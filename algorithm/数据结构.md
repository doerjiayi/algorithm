
#B+树
##B+树与LSM树浅析
https://zhuanlan.zhihu.com/p/67068975

流程看起来与B-树相差不大，但是有两点不同，首先B+树中间节点没有卫星数据，所以同样大小的磁盘页可以容纳更多的节点元素，这就意味着，在相同数据量的情况下，B+树的结构比B-树更加“矮胖”因此查询是IO次数也更少。

其次，B+树的查询必须最终查找到叶子节点，而B-树只要找到匹配元素即可，无论匹配元素处于中间节点还是叶子节点。因此B-树的查找性能并不稳定（最好情况是指查找根节点，最坏情况是查找到叶子节点）。而B+树的每一次查找都是稳定的。

综合起来B+树的特征和优势如下所示：

B+树的特征：
1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。
2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。
3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。

B+树的优势：
1.单一节点存储更多的元素，使得查询的IO次数更少。
· B+树的内部并没有指向关键字具体信息的指针，因此其内部节点相对于B树更加小，如果把所有同一内部节点的关键字存放在同一块盘中，那么盘块所能容纳的关键字数量也越多，一次性读入内存中的可供查找的关键字也就越多，相对来说IO读写次数也就下降了。
2.所有查询都要查找到叶子节点，查询性能稳定。
由于非叶子节点并不是最终指向文件内容的节点，而只是叶子节点中关键字的索引，所以任何关键字的查找必须走一条从根节点到叶子节点的路。所有关键字查询的路径长度相同，导致每一次数据的查询效率相当。
3.所有叶子节点形成有序链表，便于范围查询。
B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树的效率却非常低。

LSM和Btree差异就要在读性能和写性能进行舍和求。在牺牲的同时，寻找其他方案来弥补。
1.LSM具有批量特性,存储延迟
当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱。 多次单页随机写，变成一次多页随机写,复用了磁盘寻道时间，极大提升效率。
2.B树的写入过程
对B树的写入过程是一次原位写入的过程，主要分为两个部分，首先是查找到对应的块的位置，然后将新数据写入到刚才查找到的数据块中，然后再查找到块所对应的磁盘物理位置，将数据写入去。当然，在内存比较充足的时候，因为B树的一部分可以被缓存在内存中，所以查找块的过程有一定概率可以在内存内完成，不过为了表述清晰，我们就假定内存很小，只够存一个B树块大小的数据吧。可以看到，在上面的模式中，需要两次随机寻道（一次查找，一次原位写），才能够完成一次数据的写入，代价还是很高的。
3.LSM树放弃磁盘读性能来换取写的顺序性，似乎会认为读应该是大部分系统最应该保证的特性，所以用读换写似乎不是个好的做法，但是：
a. 内存的速度远超磁盘，1000倍以上。而读取的性能提升，主要还是依靠内存命中率而非磁盘读的次数
b. 写入不占用磁盘的io，读取就能获取更长时间的磁盘io使用权，从而也可以提升读取效率。例如LevelDb的SSTable虽然降低了了读的性能，但如果数据的读取命中率有保障的前提下，因为读取能够获得更多的磁盘io机会，因此读取性能基本没有降低，甚至还会有提升。而写入的性能则会获得较大幅度的提升，基本上是5~10倍左右.
通过以上的分析，应该知道LSM树的由来了，LSM树的设计思想非常朴素：将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，不过读取的时候稍微麻烦，需要合并磁盘中历史数据和内存中最近修改操作，所以写入性能大大提升，读取时可能需要先看是否命中内存，否则需要访问较多的磁盘文件。极端的说，基于LSM树实现的HBase的写性能比MySQL高了一个数量级，读性能低了一个数量级。
LSM树原理把一棵大树拆分成N棵小树，它首先写入内存中，随着小树越来越大，内存中的小树会flush到磁盘中，磁盘中的树定期可以做merge操作，合并成一棵大树，以优化读性能。


##B树,B+树与LSM树
https://www.jianshu.com/p/5f54a0d5daf6

###B树
B 树又叫平衡多路查找树。一棵m阶的B 树的特性如下
树中每个结点最多含有m-1个孩子（m>=2）；
除根结点和叶子结点外，其它每个结点至少有[ceil(m / 2)]个孩子（其中ceil(x)是一个取上限的函数）；
若根结点不是叶子结点，则至少有2个孩子（特殊情况：没有孩子的根结点，即根结点为叶子结点，整棵树只有一个根节点）；
所有叶子结点都出现在同一层，叶子结点不包含任何关键字信息
小结:
假如是m阶,则关键字最多有m-1个(这在页的分裂比较重要,后面会详细说),然后B树的数据是每个节点都会有的(这导致每个页可以存储的关键字数目变少,变相的导致树会变高,而树越高,磁盘寻道次数就可能越多,就越可能浪费时间)


###B+树
B+树为了控制树的高度,只在叶子节点存储数据（这样非叶子节点每一页可以存储更多的关键字,进而很可能不用下沉到叶子节点就能找到我们想要的信息,也大概率减少了寻道次数）,当然非叶子节点里面的数据是冗余的,即非叶子节点关键字是由叶子节点里面的关键字中的最小的数组成的,可以详见下面的数据结构.
备注:B+树,是假如为m阶,则每个节点最多会有m个关键字

###Mysql里面的分裂
上面我们讲的分裂都是从中间节点开始,但实际上这样会导致页空间的浪费.
比如说记录如下
1,2,3,4,5,6,7,8,9
插入式根据自增顺序进行的,若这时插入10，则会分裂为两页
P1:1,2,3,4
P2:5,6,7,8,9,10
又由于插入是顺序的,那么P1这个页将不会再有记录插入,从而导致空间浪费
那么InnoDB引擎的具体优化措施
假如现在有一页数量为5,现在插入一个数,假如该数后面还有3个数(即超过了一半),则将分裂点为插入数后3位的位置
否则分裂点就为插入数定位到的位置

前面我们分析了,当有大量分裂时,会导致大量的随机寻道,从而降低性能,所以就可以使用LSM树

###LSM树是什么呢?
它的核心思路其实非常简单，就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到最后多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。
LSM具有批量特性，存储延迟。当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱。 多次单页随机写，变成一次多页随机写,复用了磁盘寻道时间，极大提升效率。
LSM Tree弄了很多个小的有序结构，比如每m个数据，在内存里排序一次，下面100个数据，再排序一次……这样依次做下去，我就可以获得N/m个有序的小的有序结构。
在查询的时候，因为不知道这个数据到底是在哪里，所以就从最新的一个小的有序结构里做二分查找，找得到就返回，找不到就继续找下一个小有序结构，一直到找到为止。
很容易可以看出，这样的模式，读取的时间复杂度是(N/m)*log2N 。读取效率是会下降的。
当然也可以做一些优化
a、Bloom filter: 就是个带随即概率的bitmap,可以快速的告诉你，某一个小的有序结构里有没有指定的那个数据的。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里啦。效率得到了提升，但付出的是空间代价。
b、compact:小树合并为大树:因为小树他性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用log2N的方式找到，不需要再进行(N/m)*log2n的查询了

##MySQL的innodb引擎使用了B+树作为索引

B+树作为索引时，随机读很快，但是有大量的随机写时，会占用很多的磁盘IO导致消耗比较大。B+树是通过降低树的高度，使树的分叉尽可能多来达到查询时的高效率的。但是在update、insert或delete时，需要进行树的调整，因此磁盘IO的消耗会比较大。所以说B+树不适合作为leveldb和rocksdb的存储引擎。

例如：假设要写入一个100000个随机的key，对磁盘来说，最快的写入方式一定是顺序地将每一次写入都直接写入到磁盘中即可。
但这样带来的问题是查询消耗大量的磁盘IO，因为每次查询一个值都需要遍历整个数据才能找到，这个读性能就太差了；
那么如果我想获取磁盘读性能最高，应该怎么做呢？把数据全部排序就行了，B+树就是这样的结构，但B+树的写性能太差了，需要提升写，可以放弃部分磁盘读性能，怎么办呢？ 引入LSM树

LSM是如何解决问题的？

LSM树将随机写变成了append，降低了磁盘IO的消耗，但是以牺牲部分读性能达到优化写性能的目的。

将有序的分组数据划分很多个小的有序结构，比如每m个数据，在内存里排序一次，下面100个数据，再排序一次……这样依次做下去，我就可以获得N/m个有序的小的有序结构，在查询的时候，因为不知道这个数据到底是在哪里，所以就从最新的一个小的有序结构里做二分查找，找得到就返回，找不到就继续找下一个小有序结构，一直到找到为止。

因此，LSM树读取的时间复杂度是(N/m)*log2N，读取效率是会下降的，这就是LSM的根本思路。

为了降低读时磁盘IO的消耗，leveldb和rocksdb引入了bloom filter和compact机制。

LSM树是以牺牲读的效率来达到提升随机写效率的目的。

##一分钟掌握MySQL的InnoDB引擎B+树索引
https://www.cnblogs.com/sy270321/p/12904825.html

为什么InnoDB选择B+树而不是B树呢
还是上面两张图，对照着看，我们能够得出一下结论

B+树的磁盘读取代价低， 树每个节点都有data域，B+树只有叶子节才有，假设每个节点大小16KB，那么B+树比B树能存储更多的关键字，一次性读入内存的关键字的内存也会更多，B+树的高度也会比B树低,磁盘IO次数会更少。

B+树对范围查询更友好,方便遍历，B树叶子节点没有链接，而B+树叶子节点通过双向指针链接，可以很方便的进行范围查询，比如where条件中 age >=3 and age <20,,那么当找到3时就可以顺着指针找到20,而B树是不可以的。

B+树查询效率稳定性更好， 在B+树中，由于分支节点并不是最终指向文件内容的节点，分支节点只是叶子节点的索引，所以对于任意关键字的查找都必须从根节点走到分支节点，所有关键字查询路径长度相同，每个数据查询效率相当。而对于B树而言，其分支节点上也保存有数据，对于每一个数据的查询所走的路径长度是不一样的，效率也不一样，B树稳定性不如B+树好

我们仔细想想我们SQL常见的查询中，总结起来是不是也就是等于查询， 范围查询

InnoDB非主键索引怎么存储呢？
我们上面介绍的只是主键索引是这样存储的，那么非主键索引呢，其实非主键索引也B+树的，只有非主键索引的叶子节点存储的不是行记录数据，而是主键值，通过主键值再次索引获取所需要的数据。

总结一下B+树索引
采用了多叉树的结构，降低了树的高度，减少了磁盘IO次数，通过对所有叶子节点通过指针双向链接，方便的提供了遍历所有数据的特性，同时数据也是严格有序的，方便范围查询,查询效率的稳定性也非常好。

#LSM树

##LSM树原理探究
https://blog.csdn.net/weixin_34409703/article/details/93165695

合并操作
由前面的增删改查操作来看，合并操作是LSM树最重要的操作。

合并操作有两个主要的作用：

合并内存中的数据到磁盘中。
由于将内存数据合并到磁盘当中会产生大量的小的集合，并且更新和删除操作会产生大量的冗余数据，通过合并操作可以减少集合中的冗余数据并降低读操作时线性扫描的耗时。
目前广泛使用的有两种合并策略，size-tiered策略和leveled策略

size-tiered策略
size-tiered策略是HBase采用的合并策略，具体内容是当某个规模的集合达到一定的数量时，将这些集合合并为一个大的集合。比如有5个50个数据的集合，那么就将他们合并为一个250个数据的集合。这种策略有一个缺点是当集合达到一定的数据量后，合并操作会变得十分的耗时。

leveled策略
leveled策略是LevelDB和RocksDB采用的合并策略，size-tiered策略因为会产生大数据量的集合，所以会造成突发的IO和CPU资源的消耗，所以leveled策略使用了分层的数据结构来代替原来的大数据集合。

leveled策略将集合的大小限制在一个小的范围内如5MB，而且将集合划分为不同的层级。每一个层级的集合总大小是固定且递增的。如第一层为50MB，第二层为500MB...。当某一层的数据集合大小达到上限时，就会从这一层中选出一个文件和下一层合并，或者直接提升到下一层。如果在合并过程中发现了数据冲突，则丢弃下一层的数据，因为低层的数据总是更新的。

同时leveled策略会限制，除第一层外。其他的每一层的键值都不会重复。这是通过合并时剔除冗余数据实现的，以此来加速在同一层内数据的线性扫描速度。

RocksDB Compaction实例

结论
LSM树牺牲了小部分读性能，而大幅度提高了写性能，所以很适合写多读少的场景，在这种场景下比B+树更加能够胜任。通过深入了解，可以发现LSM树的合并策略会大大影响到LSM树的性能，所以应该根据具体的场景，灵活地选择相应的策略。


##LSM树
https://blog.csdn.net/las723/article/details/93767240

LSM树，即日志结构合并树(Log-Structured Merge-Tree)。其实它并不属于一个具体的数据结构，它更多是一种数据结构的设计思想。大多NoSQL数据库核心思想都是基于LSM来做的，只是具体的实现不同。所以本来不打算列入该系列，但是有朋友留言了好几次让我讲LSM树，那么就说一下LSM树。

LSM树诞生背景
传统关系型数据库使用btree或一些变体作为存储结构，能高效进行查找。但保存在磁盘中时它也有一个明显的缺陷，那就是逻辑上相离很近但物理却可能相隔很远，这就可能造成大量的磁盘随机读写。随机读写比顺序读写慢很多，为了提升IO性能，我们需要一种能将随机操作变为顺序操作的机制，于是便有了LSM树。LSM树能让我们进行顺序写磁盘，从而大幅提升写操作，作为代价的是牺牲了一些读性能。

关于磁盘IO
磁盘读写时涉及到磁盘上数据查找，地址一般由柱面号、盘面号和块号三者构成。也就是说移动臂先根据柱面号移动到指定柱面，然后根据盘面号确定盘面的磁道，最后根据块号将指定的磁道段移动到磁头下，便可开始读写。

整个过程主要有三部分时间消耗，查找时间(seek time) +等待时间(latency time)+传输时间(transmission time) 。分别表示定位柱面的耗时、将块号指定磁道段移到磁头的耗时、将数据传到内存的耗时。整个磁盘IO最耗时的地方在查找时间，所以减少查找时间能大幅提升性能。

LSM树原理
LSM树由两个或以上的存储结构组成，比如在论文中为了方便说明使用了最简单的两个存储结构。一个存储结构常驻内存中，称为C0 tree，具体可以是任何方便健值查找的数据结构，比如红黑树、map之类，甚至可以是跳表。另外一个存储结构常驻在硬盘中，称为C1 tree，具体结构类似B树。C1所有节点都是100%满的，节点的大小为磁盘块大小。



插入步骤
大体思路是：插入一条新纪录时，首先在日志文件中插入操作日志，以便后面恢复使用，日志是以append形式插入，所以速度非常快；将新纪录的索引插入到C0中，这里在内存中完成，不涉及磁盘IO操作；当C0大小达到某一阈值时或者每隔一段时间，将C0中记录滚动合并到磁盘C1中；对于多个存储结构的情况，当C1体量越来越大就向C2合并，以此类推，一直往上合并Ck。



合并步骤
合并过程中会使用两个块：emptying block和filling block。

 

从C1中读取未合并叶子节点，放置内存中的emptying block中。从小到大找C0中的节点，与emptying block进行合并排序，合并结果保存到filling block中，并将C0对应的节点删除。不断执行第2步操作，合并排序结果不断填入filling block中，当其满了则将其追加到磁盘的新位置上，注意是追加而不是改变原来的节点。合并期间如故宫emptying block使用完了则再从C1中读取未合并的叶子节点。C0和C1所有叶子节点都按以上合并完成后即完成一次合并。

关于优化措施
本文用图阐述LSM的基本原理，但实际项目中其实有很多优化策略，而且有很多针对LSM树优化的paper。比如使用布隆过滤器快速判断key是否存在，还有做一些额外的索引以帮助更快找到记录等等。



##LSM树存储模型
https://blog.csdn.net/u014774781/article/details/52105708/
LSM（log-structed-merge-tree）

leveldb和rocksdb底层使用LSM树做存储引擎，LSM树使用skiplist做索引，他们先将数据写入内存中，按照key进行划分，定期的merge写入到磁盘中，合并后数据写入下一层level

LSM是什么?解决什么问题？
在leveldb和rocksdb中，面临的一个主要问题是数据的落盘，在写磁盘时，随机写会消耗很大的磁盘IO，因此为了解决随机写的问题，引入了LSM树。LSM树将随机写变成了append，极大地降低了磁盘IO的消耗。

##由LevelDB理解 LSM-Tree

https://blog.csdn.net/double2hao/article/details/90107904

###写过程
写入log文件。（防止意外情况）
写入内存中的MemTable
内存中的MemTable大小到达一定程度后，原先的MemTable就变成了Immutable MemTable，并它只可读，不可写。LevelDB后台调用会将Immutable MemTable中的数据存储到磁盘中。
内存中创建一个新的MemTable，后续的数据写入新的这个MemTable。

###LevelDB写入磁盘过程
由上图可知，硬盘中存储的是多个.sst文件，即SSTable（Sorted String Table），SSTable的结构没有什么特别，直接把它当做一个顺序存储的数据结构就可以。

整个操作其实非常简单，直接将Immutable MemTable中的数据存储到多个SSTable，然后将这些SSTable存储到整个SStable数组的头部（也就是Level 0）就可以了。

假设此时硬盘中已经有了两个SSTable，此时硬盘中的内容为：
level 0->tableA
level 1->tableB

此时要插入一个tableC。
那么插入后的硬盘中的数据结构为：
level 0->tableC
level 1->tableA
level 2->tableB

###读过程
由上面的写入过程可以了解到，级别较低的SSTable中的内容是更新的。这时候再来看LevelDB的读过程就比较好理解了。

先去MemTable中读
如果没有读到就去Immutable MemTable中读
如果还是没有读到就到硬盘中去读，读的顺序是level从低到高。


###读过程举例
假设此刻硬盘中有两个SSTable，结构如下：
level 0->tableA
level 1->tableB
tableA中存储的值为：{“a”:123,“c”:321}
tableB中存储的值为：{“b”:222,“c”:333}

例子一
这时候要读取“a”的value，假设内存中没有读到这个值，那么硬盘中读取的顺序应该是先读取level 0，再读取level 1。
读取level 0的时候直接可以读取到"a"的value为123，于是就返回123。

例子二
这时候要读取“b”的value，假设内存中没有读到这个值，那么硬盘中读取的顺序应该是先读取level 0，再读取level 1。
先读取level 0，发现tableA中并没有“b”这个key的value，于是到level 1中去寻找，在tableB中发现“b”的value为222，于是就返回222。

不同Level的SSTable合并
levelDB会轮流让level高的文件去合并level低的文件的内容，合并完成之后，level高的文件被替换成合并后的文件，level低的文件则被删除。

比如level 0中有A.sst，level 1中有A.sst，两个文件合并后生成A_new.sst， level 1中的A.sst替换成 A_new.sst，level 0 的A.sst则删除。

由于.sst中的内容本身就是有序的，因此合并的过程就非常简单，使用多路归并排序的方式，每次将两个旧文件中最小的key记录，最终就能得到一个新的有序的SSTable，再存储为.sst文件。



#跳跃表
##漫画算法：什么是跳跃表？
https://www.jianshu.com/p/dc252b5efca6

##跳跃表的原理及实现

https://blog.csdn.net/qpzkobe/article/details/80056807

##跳跃表
https://www.cnblogs.com/Leo_wl/p/11557614.html#_label2_0


##讲讲跳跃表（Skip Lists）
https://www.cnblogs.com/Javame/p/10328685.html

##跳表SkipList
https://www.cnblogs.com/zhizhan/p/4605756.html
