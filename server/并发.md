
##多线程
https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/5950400.html
https://blog.csdn.net/chenjiayi_yun/article/details/43938623
https://blog.csdn.net/chenjiayi_yun/article/details/40427141
https://blog.csdn.net/qq_43401808/article/details/86540962
https://blog.csdn.net/chenjiayi_yun/article/details/19023135

https://www.e-learn.cn/content/qita/1141137
https://www.cnblogs.com/fudong071234/p/6759567.html
https://www.jianshu.com/p/6f5c38da6430
https://www.cnblogs.com/liang1101/p/7285955.html

https://blog.csdn.net/longbei9029/article/details/60956892

##多线程模型并发处理
###c++多线程编程：常见面试题
https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/5950400.html
####condition_variable mutex
题目：子线程循环 10 次，接着主线程循环 100 次，接着又回到子线程循环 10 次，接着再回到主线程又循环 100 次，如此循环50次，试写出代码

子线程与主线程必有一个满足条件(flag == num),不满足条件的那个线程不可能获取unique_lock(会在wait中释放)，只有满足条件的线程才能获取锁，执行程序

mutex m;//保护条件的互斥访问
condition_variable cond;//条件变量
int flag = 10;//条件
void fun(int num) {
    for (int i = 0; i<50; i++) {
        unique_lock<mutex> lk(m);//A unique lock is an object that manages a mutex object with unique ownership in both states: locked and unlocked.  
        while (flag != num)
            cond.wait(lk);//在调用wait时会执行lk.unlock()  
        for (int j = 0; j<num; j++)
            cout << j << " ";
        cout << endl;
        flag = (num == 10) ? 100 : 10;
        cond.notify_one();//被阻塞的线程唤醒后lk.lock()恢复在调用wait前的状态  
    }
}
int main() {
    thread child(fun, 10);
    fun(100);
    child.join();
    system("pause");
    return 0;
}
题目：编写一个程序，开启3个线程，这3个线程的ID分别为A、B、C，每个线程将自己的ID在屏幕上打印10遍，要求输出结果必须按ABC的顺序显示；如：ABCABC….依次递推。

mutex m;
condition_variable cond;
int loop = 10;
int flag = 0;

void func(int id)
{
    for (int i = 0; i < loop; ++i)
    {
        unique_lock<mutex> lk(m);
        while (flag != id)
            cond.wait(lk);
        cout << static_cast<char>('A' + id) << " ";
        flag = (flag + 1) % 3;
        cond.notify_all();
    }
}

void main()
{
    thread A(func, 0);
    thread B(func, 1);
    func(2);
    cout << endl;
    A.join();
    B.join();
    system("pause");
}

题目(google笔试题)：有四个线程1、2、3、4。线程1的功能就是输出1，线程2的功能就是输出2，以此类推.........现在有四个文件ABCD。初始都为空。现要让四个文件呈如下格式：
A：1 2 3 4 1 2....
B：2 3 4 1 2 3....
C：3 4 1 2 3 4....

D：4 1 2 3 4 1....

mutex m;
condition_variable cond;
int loop = 10;
int flag;

void func(int num)
{
    for (int i = 0; i < loop; ++i)
    {
        unique_lock<mutex> lk(m);
        while (num != flag)
            cond.wait(lk);
        cout << num + 1 << " ";
        flag = (flag + 1) % 4;
        cond.notify_all();
    }
}

void main(int argc,char *argv[])
{
    flag = atoi(argv[1]);
    thread one(func, 1);
    thread two(func, 2);
    thread three(func, 3);
    func(0);
    one.join();
    two.join();
    three.join();
    cout << endl;
    system("pause");
}
读者写者问题

这也是一个非常经典的多线程题目，题目大意如下：有一个写者很多读者，多个读者可以同时读文件，但写者在写文件时不允许有读者在读文件，同样有读者读时写者也不能写。

复制代码
class rwlock {
private:
    mutex _lock;
    condition_variable _wcon, _rcon;
    unsigned _writer, _reader;
    int _active;
public:
    void read_lock() {
        unique_lock<mutex> lock(_lock);
        ++_reader;
        while (_active < 0 || _writer > 0)
            _rcon.wait(lock);
        --_reader;
        ++_active;
    }
    void write_lock() {
        unique_lock<mutex> lock(_lock);
        ++_writer;
        while (_active != 0)
            _wcon.wait(lock);
        --_writer;
        _active = -1;
    }
    void unlock() {
        unique_lock<mutex> lock(_lock);
        if (_active > 0) {
            --_active;
            if (_active == 0) _wcon.notify_one();
        }
        else {
            _active = 0;
            if (_writer > 0) _wcon.notify_one();
            else if (_reader > 0) _rcon.notify_all();
        }
    }
    rwlock() :_writer(0), _reader(0), _active(0) {
    }
};

void t1(rwlock* rwl) {
    while (1) {
        cout << "I want to write." << endl;
        rwl->write_lock();
        cout << "writing..." << endl;
        this_thread::sleep_for(chrono::seconds(5));
        rwl->unlock();
        this_thread::sleep_for(chrono::seconds(5));
    }
}

void t2(rwlock* rwl) {
    while (1) {
        cout << "t2-I want to read." << endl;
        rwl->read_lock();
        cout << "t2-reading..." << endl;
        this_thread::sleep_for(chrono::seconds(1));
        rwl->unlock();
    }
}

void t3(rwlock* rwl) {
    while (1) {
        cout << "t3-I want to read." << endl;
        rwl->read_lock();
        cout << "t3-reading..." << endl;
        this_thread::sleep_for(chrono::seconds(1));
        rwl->unlock();
    }
}

int main()
{
    rwlock* rwl = new rwlock();
    thread th1(t1, rwl);
    thread th2(t2, rwl);
    thread th3(t3, rwl);
    th1.join();
    th2.join();
    th3.join();
    system("pause");
    return 0;
}
 

####线程安全的queue

STL中的queue是非线程安全的，一个组合操作：front(); pop()先读取队首元素然后删除队首元素，若是有多个线程执行这个组合操作的话，可能会发生执行序列交替执行，导致一些意想不到的行为。因此需要重新设计线程安全的queue的接口。

复制代码
template<typename T>
class threadsafe_queue
{
private:
    mutable std::mutex mut;
    std::queue<T> data_queue;
    std::condition_variable data_cond;
public:
    threadsafe_queue() {}
    threadsafe_queue(threadsafe_queue const& other)
    {
        std::lock_guard<std::mutex> lk(other.mut);
        data_queue = other.data_queue;
    }
    void push(T new_value)//入队操作  
    {
        std::lock_guard<std::mutex> lk(mut);
        data_queue.push(new_value);
        data_cond.notify_one();
    }
    void wait_and_pop(T& value)//直到有元素可以删除为止  
    {
        std::unique_lock<std::mutex> lk(mut);
        data_cond.wait(lk, [this] {return !data_queue.empty(); });
        value = data_queue.front();
        data_queue.pop();
    }
    std::shared_ptr<T> wait_and_pop()
    {
        std::unique_lock<std::mutex> lk(mut);
        data_cond.wait(lk, [this] {return !data_queue.empty(); });
        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));
        data_queue.pop();
        return res;
    }
    bool try_pop(T& value)//不管有没有队首元素直接返回  
    {
        std::lock_guard<std::mutex> lk(mut);
        if (data_queue.empty())
            return false;
        value = data_queue.front();
        data_queue.pop();
        return true;
    }
    std::shared_ptr<T> try_pop()
    {
        std::lock_guard<std::mutex> lk(mut);
        if (data_queue.empty())
            return std::shared_ptr<T>();
        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));
        data_queue.pop();
        return res;
    }
    bool empty() const
    {
        std::lock_guard<std::mutex> lk(mut);
        return data_queue.empty();
    }
};
####atomic
题目：编写程序完成如下功能：

1）有一int型全局变量g_Flag初始值为0

2） 在主线称中起动线程1，打印“this is thread1”，并将g_Flag设置为1

3） 在主线称中启动线程2，打印“this is thread2”，并将g_Flag设置为2

4） 线程序1需要在线程2退出后才能退出

5） 主线程在检测到g_Flag从1变为2，或者从2变为1的时候退出

atomic<int> flag(0);

void worker1(future<int> fut) 
{//线程1  
    printf("this is thread1\n");
    flag = 1;
    fut.get();//线程1阻塞至线程2设置共享状态  get等待异步操作结束并返回结果
    printf("thread1 exit\n");
}

void worker2(promise<int> prom) 
{//线程2  
    printf("this is thread2\n");//C++11的线程输出cout没有boost的好，还是会出现乱序，所以采用printf，有点不爽  
    flag = 2;
    prom.set_value(10);//线程2设置了共享状态后，线程1才会被唤醒  
    printf("thread2 exit\n");
}

//利用promise future来控制线程退出的次序
int main()
{
    promise<int> prom;
    future<int> fut = prom.get_future();
    thread one(worker1, move(fut));//注意future和promise不允许拷贝，但是具备move语义  
    thread two(worker2, move(prom));
    while (flag.load() == 0);
　　///将本线程从调用线程中分离出来，允许本线程独立执行
    one.detach();
    two.detach();
    //exit(1);//主线程到这里退出  
    printf("main thread exit\n");
    system("pause");
    return 0;
}

###C/C++并发编程（1）—— 并发/并行、多线程内存模型
https://www.jianshu.com/p/298296e9a887

https://blog.csdn.net/qq_40273354/article/details/78494504

##线程
http://www.cplusplus.com/reference/condition_variable/condition_variable/
http://www.cplusplus.com/reference/condition_variable/condition_variable/notify_all/
http://www.cplusplus.com/reference/condition_variable/condition_variable_any/
https://www.cnblogs.com/huty/p/8516997.html

##线程控制
https://blog.csdn.net/chenjiayi_yun/article/details/18059665
###1、Linux 线程概念
     进程与线程之间是有区别的，不过Linux内核只提供了轻量进程的支持，而其所谓的线程本质上在内核里仍然是进程。

     进程是资源分配的单位，同一进程中的多个线程共享该进程的资源。Linux中的线程只是在被创建时clone了父进程的资源，因此clone出来的进程表现为线程，只是它有共享父进程资源的特性。

   程序与线程库相链接即可支持Linux平台上的多线程，在程序中需包含头文件pthread. h，在编译链接时使用命令： 
gcc -D -REENTRANT -lpthread xxx. c

其中-REENTRANT宏使得相关库函数(如stdio.h、errno.h中函数) 是可重入的、线程安全的(thread-safe)，-lpthread则意味着链接库目录下的libpthread.a或libpthread.so文件。

流行的线程模型有LinuxThreads 和 NPTL。使用线程库需要2.0以上版本的Linux内核,及相应版本的C库(libc 或glibc )。

参考：http://www.ibm.com/developerworks/cn/linux/l-threading.html

###2、线程控制 
（1）线程创建 
进程被创建时，系统会为其创建一个主线程，而要在进程中创建新的线程，则可以调用pthread_create： 
pthread_create(pthread_t *thread, const pthread_attr_t *attr, void * (start_routine)(void*), void *arg);

start_routine为新线程的入口函数，arg为传递给start_routine的参数。 

每个线程都有自己的线程ID，以便在进程内区分。线程ID在pthread_create调用时回返给创建线程的调用者；一个线程也可以在创建后使用pthread_self()调用获取自己的线程ID： 

pthread_self (void) ;

（2）线程退出 
线程的退出方式： 
1）执行完成后隐式退出
2）由线程本身显示调用pthread_exit 函数退出
pthread_exit (void * retval) ;

3）被其他线程用pthread_cance函数终止
pthread_cance (pthread_t thread) ;

在某线程中调用此函数，可以终止由参数thread 指定的线程。 

如果一个线程要等待另一个线程的终止，可以使用pthread_join函数，该函数的作用是调用pthread_join的线程将被挂起直到线程ID为参数thread的线程终止： 

pthread_join (pthread_t thread, void** threadreturn);

###3、线程通信 
####（1）线程互斥 
互斥意味着“排它”，即两个线程不能同时进入被互斥保护的代码。Linux下可以通过pthread_mutex_t 定义互斥体机制完成多线程的互斥操作，该机制的作用是对某个需要互斥的部分，在进入时先得到互斥体，如果没有得到互斥体，表明互斥部分被其它线程拥有，此时欲获取互斥体的线程阻塞，直到拥有该互斥体的线程完成互斥部分的操作为止。 

下面的代码实现了对共享全局变量x1 用互斥体mutex 进行保护的目的： 
int x1; // 进程中的全局变量 
pthread_mutex_t mutex; 
pthread_mutex_init(&mutex, NULL); //按缺省的属性初始化互斥体变量mutex 
pthread_mutex_lock(&mutex); // 给互斥体变量加锁 
… //对变量x1 的操作 
phtread_mutex_unlock(&mutex); // 给互斥体变量解除锁

####（2）线程同步 
同步就是线程等待某个事件的发生。只有当等待的事件发生线程才继续执行，否则线程挂起并放弃处理器。当多个线程协作时，相互作用的任务必须在一定的条件下同步。 
#####1）条件变量
Linux下的C语言编程有多种线程同步机制，最典型的是条件变量(condition variable)。
pthread_cond_init用来创建一个条件变量，其函数原型为： 
pthread_cond_init (pthread_cond_t *cond, const pthread_condattr_t *attr);

pthread_cond_wait和pthread_cond_timedwait用来等待条件变量被设置，值得注意的是这两个等待调用需要一个已经上锁的互斥体mutex，这是为了防止在真正进入等待状态之前别的线程有可能设置该条件变量而产生竞争。

pthread_cond_wait的函数原型为： 
pthread_cond_wait (pthread_cond_t *cond, pthread_mutex_t *mutex);


pthread_cond_broadcast用于设置条件变量，即使得事件发生，这样等待该事件的线程将不再阻塞： 
pthread_cond_broadcast (pthread_cond_t *cond) ;
pthread_cond_signal则用于解除某一个等待线程的阻塞状态： 
pthread_cond_signal (pthread_cond_t *cond) ;

pthread_cond_destroy(pthread_cond_t *cond)  则用于释放一个条件变量的资源。 
pthread_cond_destroy(pthread_cond_t *cond) ；

pthread_cond_timedwait 计时等待方式

int pthread_cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *mutex, const struct timespec *abstime);

如果在给定时刻前条件没有满足，则返回ETIMEOUT，结束等待，其中abstime以与time()系统调用相同意义的绝对时间形式出现，0表示格林尼治时间1970年1月1日0时0分0秒。

pthread_cond_timedwait 和pthread_cond_wait，都必须和一个互斥锁配合，以防止多个线程同时请求pthread_cond_wait()（或pthread_cond_timedwait()，下同）的竞争条件（Race Condition）。mutex互斥锁必须是普通锁（PTHREAD_MUTEX_TIMED_NP）或者适应锁（PTHREAD_MUTEX_ADAPTIVE_NP），且在调用pthread_cond_wait()前必须由本线程加锁（pthread_mutex_lock()），而在更新条件等待队列以前，mutex保持锁定状态，并在线程挂起进入等待前解锁。在条件满足从而离开pthread_cond_wait()之前，mutex将被重新加锁，以与进入pthread_cond_wait()前的加锁动作对应。
激发条件有两种形式，pthread_cond_signal()激活一个等待该条件的线程，存在多个等待线程时按入队顺序激活其中一个；而pthread_cond_broadcast()则激活所有等待线程。


条件变量实例1：
以名的生产者/消费者问题为例来阐述Linux线程的控制和通信。一组生产者线程与一组消费者线程通过缓冲区发生联系。生产者线程将生产的产品送入缓冲区，消费者线程则从中取出产品。缓冲区有N 个，是一个环形的缓冲池。 

// 缓冲区相关数据结构
struct prodcons
{
    int buffer[BUFFER_SIZE]; /* 实际数据存放的数组*/
    pthread_mutex_t lock; /* 互斥体lock 用于对缓冲区的互斥操作 */
    int readpos, writepos; /* 读写指针*/
    pthread_cond_t notempty; /* 缓冲区非空的条件变量（写者通知读者的信号） */
    pthread_cond_t notfull; /* 缓冲区未满的条件变量 （读者通知写者的信号）*/
};
 
/* 初始化缓冲区结构 */
void init(struct prodcons *b)
{
    pthread_mutex_init(&b->lock, NULL);
    pthread_cond_init(&b->notempty, NULL);
    pthread_cond_init(&b->notfull, NULL);
    b->readpos = 0;
    b->writepos = 0;
}
/* 将产品放入缓冲区,这里是存入一个整数*/
void put(struct prodcons *b, int data)
{
    pthread_mutex_lock(&b->lock);
    /* 等待缓冲区未满*/
    if ((b->writepos + 1) % BUFFER_SIZE == b->readpos)
    {
        pthread_cond_wait(&b->notfull, &b->lock);//数据已满，等待读者发送信号
    }
    /* 写数据,并移动指针 */
    b->buffer[b->writepos] = data;
    b->writepos++;
    if (b->writepos >= BUFFER_SIZE)
        b->writepos = 0;
    /* 设置缓冲区非空的条件变量*/
    if (b->writepos == (b->readpos + 1) % BUFFER_SIZE)//判断通知条件
    pthread_cond_signal(&b->notempty);//放了一个数据，就通知一次读者
    pthread_mutex_unlock(&b->lock);
} 
/* 从缓冲区中取出整数*/
int get(struct prodcons *b)
{
    int data;
    pthread_mutex_lock(&b->lock);
    /* 等待缓冲区非空*/
    if (b->writepos == b->readpos)
    {
        pthread_cond_wait(&b->notempty, &b->lock);//读者等待写者发送信号
    }
    /* 读数据,移动读指针*/
    data = b->buffer[b->readpos];
    b->readpos++;
    if (b->readpos >= BUFFER_SIZE)
        b->readpos = 0;
    /* 设置缓冲区未满的条件变量*/
    if ((b->writepos + 2) % BUFFER_SIZE == b->readpos)//判断通知条件
    pthread_cond_signal(&b->notfull);//读者发送信号给写者
    pthread_mutex_unlock(&b->lock);
    return data;
}
 
/* 测试:生产者线程将1 到10000 的整数送入缓冲区,消费者线
   程从缓冲区中获取整数,两者都打印信息*/
int OVER ( - 1)
struct prodcons buffer;
void *producer(void *data)
{
    int n;
    for (n = 0; n < 10000; n++)
    {
        printf("%d --->\n", n);
        put(&buffer, n);
    }
    put(&buffer, OVER);
    return NULL;
}
 
void *consumer(void *data)
{
    int d;
    while (1)
    {
        d = get(&buffer);
        if (d == OVER)
            break;
        printf("--->%d \n", d);
    }
    return NULL;
}
 
int main(void)
{
    pthread_t th_a, th_b;
    void *retval;
    init(&buffer);
    /* 创建生产者和消费者线程*/
    pthread_create(&th_a, NULL, producer, 0);
    pthread_create(&th_b, NULL, consumer, 0);
    /* 等待两个线程结束*/
    pthread_join(th_a, &retval);
    pthread_join(th_b, &retval);
    return 0;
}

条件变量实例2：

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
 
struct node
{
    intn_number;
    structnode *n_next;
} *head = NULL; /*[thread_func]*/
 
 
/*释放节点内存 */
static void cleanup_handler(void*arg)
{
    printf("Cleanup handler of second thread.\n");
    free(arg);
    pthread_mutex_unlock(&mtx);
}
 
static void* thread_func(void*arg)
{
    structnode *p = NULL;
    pthread_cleanup_push(cleanup_handler, p);//pthread_cond_wait是取消点，所以需要在被取消时解锁
    while(1)
    {
        pthread_mutex_lock(&mtx);
        //这个mutex_lock主要是用来保护wait等待临界时期的情况，
        //当在wait为放入队列时，这时，已经存在Head条件等待激活
        //的条件，此时可能会漏掉这种处理
        //这个while要特别说明一下，单个pthread_cond_wait功能很完善，
        //为何这里要有一个while (head == NULL)呢？因为pthread_cond_wait
        //里的线程可能会被意外唤醒，如果这个时候head == NULL，则
        //则不是我们想要的情况。这个时候，
        //应该让线程继续进入pthread_cond_wait
        //（本例逻辑上使用if while(head == NULL)也可以达到目的，使用while(head == NULL)是为了编码安全的习惯）
        while(head == NULL)
        {
            pthread_cond_wait(&cond, &mtx);
            // pthread_cond_wait会先解除之前的pthread_mutex_lock锁定的mtx，
            //然后阻塞在等待队列里休眠，直到再次被唤醒
            //（大多数情况下是等待的条件成立而被唤醒，唤醒后，
            //该进程会先锁定先pthread_mutex_lock(&mtx);，
            // 再读取资源 用这个流程是比较清楚的
            /*block-->unlock-->wait() return-->lock*/
        }
	      p = head;
        head = head->n_next;
        printf("Got %d from front of queue\n", p->n_number);
        free(p);
        pthread_mutex_unlock(&mtx);//临界区数据操作完毕，释放互斥锁
    }
    pthread_cleanup_pop(0);
    return0;
}
 
int main(void)
{
    pthread_t tid;
    inti;
    structnode *p;
    pthread_create(&tid, NULL, thread_func, NULL);
    //子线程会一直等待资源，类似生产者和消费者，
    //但是这里的消费者可以是多个消费者，
    //而不仅仅支持普通的单个消费者，这个模型虽然简单，但是很强大
    for(i = 0; i < 10; i++)
    {
        p = (struct node *)malloc(sizeof(struct node));//分配新节点
        p->n_number = i;
        pthread_mutex_lock(&mtx);//需要操作head这个临界资源，先加锁(需要修改共享资源的才在pthread_cond_signal前后加锁，并把资源操作放置在临界区内)
        p->n_next = head;//放到链表头部
        head = p;
        pthread_cond_signal(&cond);//有数据就通知读者
        pthread_mutex_unlock(&mtx);//解锁
        sleep(1);
    }
    printf("thread 1 wanna end the cancel thread 2.\n");
    pthread_cancel(tid);
    //关于pthread_cancel，有一点额外的说明，它是从外部终止子线程，
    //子线程会在最近的取消点，退出线程，而在我们的代码里，最近的
    //取消点肯定就是pthread_cond_wait()了。
    pthread_join(tid, NULL);
    printf("All done -- exiting\n");
    return0;
}

#####2）信号量
在头文件semaphore.h 中定义的信号量则完成了互斥体和条件变量的封装，按照多线程程序设计中访问控制机制，控制对资源的同步访问，提供程序设计人员更方便的调用接口。 

sem_init(sem_t *sem, int pshared, unsigned int val);
这个函数初始化一个信号量sem 的值为val，参数pshared 是共享属性控制，表明是否在进程间共享。 

sem_wait(sem_t *sem);

调用该函数时，若sem为无状态，调用线程阻塞，等待信号量sem值增加(post )成为有信号状态；若sem为有状态，调用线程顺序执行，但信号量的值减一。

sem_post(sem_t *sem); 

调用该函数，信号量sem的值增加，可以从无信号状态变为有信号状态。
 

###4、WIN32、Linux线程函数比较

对于win32和linux的线程控制和线程通信函数，本质内容一致。接口如下：

事项	WIN32	Linux	 
线程创建	CreateThread	pthread_create	 
线程终止	执行完成后退出；线程自身调用ExitThread函数即终止自己；被其他线程调用函数TerminateThread函数	执行完成后退出；由线程本身调用pthread_exit 退出；被其他线程调用函数pthread_cance终止	 
获取线程ID	GetCurrentThreadId	pthread_self	 
创建互斥	CreateMutex	pthread_mutex_init	 
获取互斥	WaitForSingleObject、WaitForMultipleObjects	pthread_mutex_lock	 
释放互斥	ReleaseMutex	phtread_mutex_unlock	 
创建信号量	CreateSemaphore	sem_init	 
等待信号量	WaitForSingleObject	sem_wait	 
释放信号量	ReleaseSemaphore	sem_post	

##线程池
https://github.com/lzpong/threadpool/blob/master/threadpool.h



##读写锁
https://www.cnblogs.com/i80386/p/4478021.html
https://www.cnblogs.com/i80386/p/4478021.html
https://www.cnblogs.com/defen/p/4410232.html
https://blog.csdn.net/yand789/article/details/27324295

##atomic
http://www.cplusplus.com/reference/atomic/atomic/exchange/
https://blog.csdn.net/FreeeLinux/article/details/53695111
https://www.cnblogs.com/dengzz/p/5686866.html

##原子
https://blog.csdn.net/chenjiayi_yun/article/details/16333779

##thread_local
###C++11多线程-线程局部存储(thread_local)
https://www.jianshu.com/p/8df45004bbcb

线程局部存储在其它语言中都是以库的形式提供的(库函数或类)。但在C++11中以关键字的形式，做为一种存储类型出现，由此可见C++11对线程局部存储的重视。C++11中有如下几种存储类型:

序号	类型	备注
1	auto	该关键字用于两种情况：
1. 声明变量时： 根据初始化表达式自动推断变量类型。
2. 声明函数作为函数返回值的占位符。
2	static	static变量只初始化一次，除此之外它还有可见性的属性：
1. static修饰函数内的“局部”变量时，表明它不需要在进入或离开函数时创建或销毁。且仅在函数内可见。
2. static修饰全局变量时，表明该变量仅在当前(声明它的)文件内可见。
3. static修饰类的成员变量时，则该变量被该类的所有实例共享。
3	register	寄存器变量。该变量存储在CPU寄存器中，而不是RAM(栈或堆)中。该变量的最大尺寸等于寄存器的大小。由于是存储于寄存器中，因此不能对该变量进行取地址操作。
4	extern	引用一个全局变量。当在一个文件中定义了一个全局变量时，就可以在其它文件中使用extern来声明并引用该变量。
5	mutable	仅适用于类成员变量。以mutable修饰的成员变量可以在const成员函数中修改。参见上一章chan.simple.h中对mutex的使用。
6	thread_local	线程周期
thread_local修饰的变量具有如下特性:

变量在线程创建时生成(不同编译器实现略有差异，但在线程内变量第一次使用前必然已构造完毕)。
线程结束时被销毁(析构，利用析构特性，thread_local变量可以感知线程销毁事件)。
每个线程都拥有其自己的变量副本。
thread_local可以和static或extern联合使用，这将会影响变量的链接属性。
下面代码演示了thread_local变量在线程中的生命周期

// thread_local.cpp

class A {
public:
  A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }

  ~A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }

  // 线程中，第一次使用前初始化
  void doSth() {
  }
};

thread_local A a;

int main() {
  a.doSth();
  std::thread t([]() {
    std::cout << "Thread: "
              << std::this_thread::get_id()
              << " entered" << std::endl;
    a.doSth();
  });

  t.join();

  return 0;
}
运行该程序

$> g++ -std=c++11 -o debug/tls.out ./thread_local.cpp
$> ./debug/tls.out
01 A(0xc00720)
Thread: 02 entered
02 A(0xc02ee0)
02 ~A(0xc02ee0)
01 ~A(0xc00720)
$>
变量a在main线程和t线程中分别保留了一份副本，以下时序图表明了两份副本的生命周期。


https://blog.csdn.net/y396397735/article/details/81271339
https://blog.csdn.net/woshi_caibi/article/details/71124390

##yield
https://blog.csdn.net/ldw614/article/details/79924587
https://blog.csdn.net/nirendao/article/details/51628693

##协程
https://www.jianshu.com/p/837bb161793a

第2章.协程库的设计与实现

4.框架级
以100%行为模拟的方式HOOK了网络io相关的syscall，可以完全不改代码兼容大多数第三方库；依照专为协程而生的语言的使用经验，提供了协程开发所必须的完整生态；

代表作：libgo

这一层次的协程库，能够100%模拟被hook的syscall的行为，能够兼容任何网络io行为的同步模型的第三方库；由于协程开发生态的完善，对开发人员的要求变得很低，新手也可以写出高效稳定的代码。但由于C++的灵活性，用户行为是不受限的，所以依然存在几个边边角角的难点需要开发者注意：没有gc（开发者要了解协程的调度时机和生命期），TLS的问题，用户不按套路出牌、把逻辑代码run在协程之外，粗粒度的线程锁等等。

第2节.协程栈
我们通常会创建数量非常庞大的协程来支持高并发，协程栈内存占用情况就变成一个不容忽视的问题了；

如果采用线程栈相同的大栈方案（linux系统默认8MB），启动1000个协程就要8GB内存，启动10w个协程就要800GB内存，而每个协程真正使用的栈内存可以几百kb甚至几kb，内存使用率极低，这显然是不可接受的；

如果采用减少协程栈的大小，比如设为128kb，启动1000个协程要128MB内存，启动10w个协程要12.8GB内存，这是一个合理的设置；但是，我们知道有很多人喜欢直接在栈上申请一个64kb的char数组做缓冲区，即使开发者非常小心的不这样奢侈的使用栈内存，也难免第三方库做这样的行为，而只需两层嵌套就会栈溢出了。

栈内存不可太大，也不可太小，这其中是很难权衡的，一旦定死这个值，就只能针对特定的场景，无法做到通用化了； 针对协程栈的内存问题，一般有以下几种方案。

静态栈(Static Stack)

固定大小的栈，存在上述的难以权衡的问题；

但是如果把问题限定在某一个范围，比如说我就只用来写微信后台、并且严格review每一个引入的第三方库的源码，确保其全部谨慎使用栈内存，这种方案也是可以作为实际项目来使用的。

典型代表：libco，它设置了128KB大小的堆栈，15年的时候我们把它引入我们当时的项目中，其后出现过多次栈溢出的问题。

分段栈(Segmented Stack)

gcc提供的“黄金链接器”支持一种允许栈内存不连续的编译参数，实现原理是在每个函数调用开头都插入一段栈内存检测的代码，如果栈内存不够用了就申请一块新的内存，作为栈内存的延续。

这种方案本应是最佳的实现，但如果遇到的第三方库没有使用这种方式来编译(注意:glibc也是这里提到的”第三方库")，那就无法在其中检测栈内存是否需要扩展，栈溢出的风险很大。

拷贝栈(Copy Stack)

每次检测到栈内存不够用时，申请一块更大的新内存，将现有的栈内存copy过去，就像std::vector那样扩展内存。

在某些语言上是可以实现这样的机制，但C++ 是有指针的，栈内存的Copy会导致指向其内存地址的指针失效；又因为其指针的灵活性(可以加减运算)，修改对应的指针成为了一种几乎不可能实现的事情(参照c++ 为什么没办法实现gc原理,详见《C++11新特性解析与应用》第5章 5.2.4节)。

共享栈(Shared Stack)

申请一块大内存作为共享栈(比如：8MB)，每次开始运行协程之前，先把协程栈的内存copy到共享栈中，运行结束后再计算协程栈真正使用的内存，copy出来保存起来，这样每次只需保存真正使用到的栈内存量即可。

这种方案极大程度上避免了内存的浪费，做到了用多少占多少，同等内存条件下，可以启动的协程数量更多，

libco

使用这种方案单机启动了上千万协程。

但是这种方案的缺陷也同样明显：

1.协程切换慢：每次协程切换，都需要2次Copy协程栈内存，这个内存量基本上都在1KB以上，通常是几十kb甚至几百kb，这样的2次Copy要花费很长的时间。

2.栈上引用失效导致隐蔽的bug：例如下面的代码


点击此处添加图片说明文字

​bar这个协程函数里面，启动了一个新的协程，然后bar等待新协程结束后再退出；当切换到新协程时，由于bar协程的栈已经被copy到了其他位置，栈上分配的变量a已经失效，此时调用a.foo就会出现难以预料的结果。

这样的场景在开发中数不胜数，比如：某个处理流程需要聚合多个后端的结果、父协程对子协程做一些计数类的操作等等等等

有人说我可以把变量a分配到堆上，这样的改法确实可以解决这个已经发现的bug；那其他没发现的怎么办呢，难道每个变量都放到堆上以提前规避这个坑？这显然是不切实际的。

早期的libgo也使用过共享栈的方式，也正是因为作者在实际开发中遇到了这样的问题，才放弃了共享栈的方式。

虚拟内存栈(Virtual Memory Stack)

既然前面提到的4种协程栈都有这样那样的弊端，那么有没有一种方案能够相对完美的解决这个问题？答案就是虚拟内存栈。

Linux、Windows、MacOS三大主流操作系统都有这样一个虚拟内存机制：进程申请的内存并不会立即被映射成物理内存，而是仅管理于虚拟内存中，真正对其读写时会触发缺页中断，此时才会映射为物理内存。

比如：我在进程中malloc了1MB的内存，但是不做读写，那么物理内存占用是不会增加的；当我读写这块内存的第一个字节时，系统才会将这1MB内存中的第一页(默认页大小4KB)映射为物理内存，此时物理内存的占用会增加4KB，以此类推，可以做到用多少占多少，冗余不超过一个内存页大小。

基于这样一个机制，libgo为每个协程malloc 1MB的虚拟内存作为协程栈(这个值是可以定制化的)；不做读写操作就不会占用物理内存，协程栈使用了多少才会占用多少物理内存，实现了与共享栈近似的内存使用率，并且不存在共享栈的两大弊端。

典型代表：

libgo

第3节.协程调度
像操作系统的进程调度一样，协程调度也有多种方案可选，也有公平调度和不公平调度之分。

栈式调度

栈式调度是典型的不公平调度：协程队列是一个栈式的结构，每次创建的协程都置于栈顶，并且会立即暂停当前协程并切换至子协程中运行，子协程运行结束(或其他原因导致切换出来)后，继续切换回来执行父协程；越是处于栈底部的协程(越早创建的协程)，被调度到的机会越少；

甚至某些场景下会产生隐晦的死循环导致永远在栈顶的两个协程间切来切去，其他协程全部无法执行。

典型代表：

libco

星切调度(非对称协程调度)

调度线程 -> 协程A -> 调度线程 -> 协程B -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来就像是星星一样，因此戏称为星切。

将当前可调度的协程组织成先进先出的队列(runnable list)，顺序pop出来做调度；新创建的协程排入队尾，调度一次后如果状态依然是可调度(runnable)的协程则排入队尾，调度一次后如果状态变为阻塞，那阻塞事件触发后也一样排入队尾，是为公平调度。

典型代表：

libgo

环切调度(对称协程调度)

调度线程 -> 协程A -> 协程B -> 协程C -> 协程D -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来呈环状，因此戏称为环切。

从调度顺序上可以发现，环切的切换次数仅为星切的一半，可以带来更高的整体切换速度；但是多线程调度、WorkSteal方面会带来一定的挑战。

这种方案也是libgo后续优化的一个方向

多线程调度、负载均衡与WorkSteal

本节的内容其实不是协程库的必选项，互联网服务端开发领域现在主流方案都是微服务，单线程多进程的模型不会有额外的负担。

但是某些场景下多进程会有很昂贵的额外成本(比如：开发一个数据库)，只能用多线程来解决，libgo为了有更广阔的适用性，实现了多线程调度和Worksteal。同时也突破了传统协程库仅用来处理网络io密集型业务的局限，也能适用于cpu密集型业务，充当并行编程库来使用。

libgo的多线程调度采用N:M模型，调度线程数量可以动态增加，但不能减少； 每个调度线程持有一个Processer(后文简称: P)，每个P持有3个runnable协程队列(普通队列、IO触发队列、亲缘性队列)，其中普通队列保存的是可以被偷取的协程；当某个P空闲时，会去其他P的队列尾部偷取一些协程过来执行，以此实现负载均衡。

为了IO方面降低线程竞争，libgo会为每个调度线程在必要的时候单独创建一个epoll；

关于每个epoll的使用，会在后面的本章第4节.HOOK-网络io中展开详细论述；其他关于多线程的设计会贯穿全文的逐个介绍。

​定时器

libgo框架的主调度器提供了一个基于红黑树的定时器，会在调度线程的主循环中被执行，这样的设计可以与epoll更好地协同工作，无论是定时器还是epoll监听的fd都可以最及时的触发。

使用co_timer_add接口可以添加一个定时任务，co_timer_add接口接受两个参数，第一个参数是可以是std::chrono::system_clock::time_point，也可以是std::chrono::steady_clock::time_point，还可以是std::chrono库里的一个duration。第二个参数接受一个回调函数，可以是函数指针、仿函数、lambda等等；

当第一个参数使用system_clock::time_point时，表示定时任务跟随系统时间的变化而变化，可以通过调整操作系统的时间设置提前或延缓定时任务的执行。

当第一个参数使用另外两种类型时，定时任务不随系统时间的变化而变化。

co_timer_add接口返回一个co::TimerId类型的定时任务id，可以用来取消定时任务。

取消定时任务有种方式：co_timer_cancel和co_timer_block_cancel，均会返回一个bool类型表示是否取消成功。

使用co_timer_cancel，会立即返回，即使定时任务正在被执行。

使用co_timer_block_cancel，如果定时任务正在被执行，则会阻塞地等待任务完成后返回false；否则会立即返回；

需要注意的是co_timer_block_cancel的阻塞行为是使用自旋锁实现的，如果定时任务耗时较长，co_timer_block_cancel的阻塞行为不但会阻塞当前调度线程，还会产生高昂的cpu开销；这个接口是设计用来在libgo内部使用的，请用户谨慎使用！

CLS(Coroutine Local Storage)(协程本地存储)

CLS类似于TLS(Thread Local Storage)；

这个功能是HOOK DNS函数族的基石，没有CLS的协程库是无法HOOK DNS函数族的。

libgo

提供了一个行为是TLS超集的CLS功能，CLS变量可以定义在全局作用域、块作用域(函数体内)、类的静态成员，除此TLS也支持的这三种场景外，还可以作为类的非静态成员。

注：

libco

也有CLS功能，但是仅支持全局作用域

CLS的使用方式参见tutorail文件夹下的sample13_cls.cpp教程代码。

线程池

除了前文提到的各种边角问题之外，还有一个非常常见的边角问题：文件IO 笔者曾经努力尝试过HOOK文件IO操作，但很不幸linux系统中，文件fd是无法使用poll、select、epoll正确监听可读可写状态的；linux提供的异步文件IO系统调用nio又不支持操作系统的文件缓存，不适合用来实现HOOK(这会导致用户的所有文件IO都不经过系统缓存而直接操作硬盘，这是一种不恰当的做法)。

除此之外也还会有其他不能HOOK或未被HOOK的阻塞syscall，因此需要一个线程池机制来解决这种阻塞行为对协程调度的干扰。

libgo提供了一个宏：co_await，来辅助用户完成线程池与协程的交互。


​在协程中使用


​可以把func投递到线程池中，并且挂起当前协程，直到func完成后协程会被唤醒，继续执行下去。 也可以使用


​等待bar在线程池中完成，并将bar的返回值写入变量a中。 co_await也同样可以在协程之外被调用。

另外，为了用户更灵活的定制线程数量，也为了libgo不偷起后台线程的操守；线程池并不会自行启动，需要用户自行启动一个或多个线程执行co_sched.GetThreadPool().RunLoop();

调试

libgo作为框架级的协程库，调试机制是必不可少的。

1.可以设置co_sched.GetOptions().debug打印一些log，具体flag见config.h

2.可以设置一个协程事件监听器，详见tutorial文件夹下的sample12_listener.cpp教程代码

3.编译时添加cmake参数：-DENABLE_DEBUGGER=ON 开启debug信息收集后，可以使用co::CoDebugger类获取一些调试信息，详见debugger.h的注释

4.后续还会提供更多调试手段

协程之外(运行在线程上的代码)

前文提到了很多功能都可以在线程上执行：Channel、co_await、co_mutex、定时器、CLS

跨平台

libgo支持三大主流系统：linux、windows、mac-os

linux是主打平台，也是libgo运行性能最好的平台，master分支永远支持linux

win分支支持windows系统，会不定期的将master分支的新功能合入其中

mac的情况同windows

（个人开发者精力有限，还请见谅！）

上层封装

笔者另有一个开源库：libgonet，是基于libgo封装的linux协程网络库，使用起来极为方便。

如果你要开发一个网络服务或rpc框架，更推荐从libgonet写起，毕竟即使有协程，socket相关的处理也并不轻松。

未来的发展方向

1.目前是使用go、go_stack、go_dispatch三个不同的宏来设置协程的属性，这种方式不够灵活，后续要改成： go stack(1024 * 1024) dispatch(::co::egod_robin) func; 这样的语法形式，可以更灵活的定制协程属性。

2.基于(1)的新语法，实现“协程亲缘性”功能，将协程绑定到指定线程上，并防止被steal。

3.优化协程切换速度：

A）使用环切调度替代现在的星切调度(CoYeild时选择下一个切换目标)，必要时才切换回线程处理epoll、定时器、sleep等逻辑，同时协调好多线程调度

B）调度器的Run函数里面做了很多协程切换之外的事情，尽量降低这部分在非必要时的cpu消耗，比如：有任务加入定时器是设置一个tls标记为true，只有标记为true时才去处理定时器相关逻辑。

C）调度器中的runnable队列使用了自旋锁，没有竞争时对原子变量的操作也是比较昂贵的，runnable队列可以优化成多写一读，仅在写入端加锁的队列。

4.协程对象Task内存布局调优，tls池化，每个池使用多写一读链表队列，申请时仅在当前线程的池中申请，可以免锁，释放时均衡每个线程的池水水位，可以塞入其他线程的池中。

5.libgo之外，会进一步寻找和当前已经比较成熟的非协程的开发框架的结合方案，让还未能用上协程的用户低成本的用上协程。

libgo开源地址:https://github.com/yyzybb537/libgo

https://www.cnblogs.com/sniperHW/archive/2012/08/05/2624334.html
https://www.cnblogs.com/heluan/p/9689751.html

##context
寄存器
https://www.jianshu.com/p/57128e477efb
make_fcountext、jump_fcontext
https://segmentfault.com/a/1190000019154852

协程分析之context上下文切换
https://blog.csdn.net/waruqi/article/details/53201416
https://www.liangzl.com/get-article-detail-578.html
https://blog.csdn.net/libaineu2004/article/details/80554870
https://blog.csdn.net/zdyueguanyun/article/details/60782260
https://gitee.com/DreamThat/libgo
https://www.jianshu.com/p/43216b429583

##阻塞非阻塞 异步同步
http://www.sohu.com/a/231730171_231667

###Unix网络编程中的五种IO模型

Blocking IO - 阻塞IO
NoneBlocking IO - 非阻塞IO
IO multiplexing - IO多路复用
signal driven IO - 信号驱动IO
asynchronous IO - 异步IO

由于signal driven IO在实际使用中并不常用，所以这里只讨论剩下的四种IO模型。

在讨论之前先说明一下IO发生时涉及到的对象和步骤，对于一个network IO，它会涉及到两个系统对象：

application 调用这个IO的进程
kernel 系统内核
那他们经历的两个交互过程是：

阶段1 wait for data 等待数据准备
阶段2 copy data from kernel to user 将数据从内核拷贝到用户进程中
之所以会有同步、异步、阻塞和非阻塞这几种说法就是根据程序在这两个阶段的处理方式不同而产生的。了解了这些背景之后，我们就分别针对四种IO模型进行讲解

###Blocking IO - 阻塞IO

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概如下图：

当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。对于network IO来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

所以，blocking IO的特点就是在IO执行的两个阶段都被block了。

###NoneBlockingIO - 非阻塞IO

linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：


从图中可以看出，当用户进程发出recvfrom这个系统调用后，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个结果（no datagram ready）。从用户进程角度讲 ，它发起一个操作后，并没有等待，而是马上就得到了一个结果。用户进程得知数据还没有准备好后，它可以每隔一段时间再次发送recvfrom操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

所以，用户进程其实是需要不断的主动询问kernel数据好了没有。

###IO multiplexing - IO多路复用
I/O多路复用(multiplexing)是网络编程中最常用的模型，像我们最常用的select、epoll都属于这种模型。以select为例：

看起来它与blocking I/O很相似，两个阶段都阻塞。但它与blocking I/O的一个重要区别就是它可以等待多个数据报就绪（datagram ready），即可以处理多个连接。这里的select相当于一个“代理”，调用select以后进程会被select阻塞，这时候在内核空间内select会监听指定的多个datagram (如socket连接)，如果其中任意一个数据就绪了就返回。此时程序再进行数据读取操作，将数据拷贝至当前进程内。由于select可以监听多个socket，我们可以用它来处理多个连接。

在select模型中每个socket一般都设置成non-blocking，虽然等待数据阶段仍然是阻塞状态，但是它是被select调用阻塞的，而不是直接被I/O阻塞的。select底层通过轮询机制来判断每个socket读写是否就绪。

当然select也有一些缺点，比如底层轮询机制会增加开销、支持的文件描述符数量过少等。为此，Linux引入了epoll作为select的改进版本。

###asynchronous IO - 异步IO

异步I/O在网络编程中几乎用不到，在File I/O中可能会用到：

这里面的读取操作的语义与上面的几种模型都不同。这里的读取操作(aio_read)会通知内核进行读取操作并将数据拷贝至进程中，完事后通知进程整个操作全部完成（绑定一个回调函数处理数据）。读取操作会立刻返回，程序可以进行其它的操作，所有的读取、拷贝工作都由内核去做，做完以后通知进程，进程调用绑定的回调函数来处理数据。

https://blog.csdn.net/zh13544539220/article/details/44856363

##高性能网络编程7--tcp连接的内存使用
https://cloud.tencent.com/developer/article/1345061

##大并发下TCP内存消耗优化小记（86万并发业务正常服务） 
https://www.cnblogs.com/x_wukong/p/7998903.html

TCP能够使用的内存：这三个值就是TCP使用内存的大小，单位是页，每个页是4K的大小，如下：
 
这三个值分别代表
Low：6179424   （6179424*4/1024/1024大概23g）
Pressure：8239232 （8239232*4/1024/1024大概31g）
High：12358848   （echo 12358848*4/1024/1024大概47g）

这个也是系统装后的默认取值，也就是说最大有47个g（75%的内存）可以用作TCP连接，这三个量也同时代表了三个阀值，TCP的使用小于第二个值时kernel不会有任何提示操作，当大于第二个值时进入压力模式，当高于第三个值时将不接受新的TCP连接，同时会报出“Out  of  socket memory”或者“TCP:too many of orphaned sockets”。

TCP读缓存大小，单位是字节：第一个是最小值4K，第二个是默认值85K，第三个是最大值16M

##tcp内存占用/socket内存占用 
https://www.cnblogs.com/shengulong/p/11623621.html

##Linux 中每个 TCP 连接最少占用多少内存？
https://zhuanlan.zhihu.com/p/25241630
https://www.ctolib.com/topics-110215.html

##HTTP性能极限优化
https://blog.csdn.net/russell_tao/article/details/103952639#comments

##详解HTTP2四大核心特性
https://developer.51cto.com/art/201910/604255.htm

#libgo 
##libgo github
https://github.com/yyzybb537/libgo/blob/master/tutorial/sample14_defer.cpp

##libgo libco 魅族架构
Kiev框架简介
kiev是魅族科技推送平台目前使用的Linux-C++后台开发框架。从2012年立项起，先后由多位魅族资深架构师、资深C++工程师倾力打造，到本文写就的时间为止，已经在推送平台这个千万用户级的大型分布式系统上经历了近5年的考验。如今Kiev在魅族推送平台中，每天为上百个服务完成数百亿次RPC调用。

kiev作为一套完整的开发框架，是专为大型分布式系统后台打造的C++开发框架，由以下几个组件组成：

RPC框架(TCP/UDP)
FastCGI框架
redis客户端(基于hiredis封装)
mysql客户端(基于mysqlclient封装)
mongodb客户端
配置中心客户端(Http协议, 基于curl实现)
基于zookeeper的分布式组件(服务发现、负载均衡)
日志模块
状态监控模块
核心模块是一个开源的`CSP并发模型`协程库(libgo)
并发模型
Kiev采用了很先进的CSP开发模型的一个变种(golang就是这种模型)，这一模型是继承自libgo的。 选择这种模型的主要原因是这种模型的开发效率远高于异步回调模型，同时不需要在性能上做出任何妥协，在文中会对常见的几种模型做详细的对比。

CSP模型
CSP(Communicating Sequential Process)模型是一种目前非常流行的并发模型，golang语言所采用的并发模型就是CSP模型。 在CSP模型中，协程与协程间不直接通信，也不像Actor模型那样直接向目标协程投递信息，而是通过一个Channel来交换数据。

RPC框架
RPC（Remote Procedure Call）是一种远程调用协议，简单地说就是能使应用像调用本地方法一样的调用远程的过程或服务，可以应用在分布式服务、分布式计算、远程服务调用等许多场景。说起 RPC 大家并不陌生，业界有很多开源的优秀 RPC 框架，例如 Dubbo、Thrift、gRPC、Hprose 等等。 RPC框架的出现是为了简化后台内部各服务间的网络通讯，让开发人员可以专注于业务逻辑，而不必与复杂的网络通讯打交道。 在我们看来，RPC框架绝不仅仅是封装一下网络通讯就可以了的，要想应对数以百计的不同服务、数千万用户、百亿级PV的业务量挑战，RPC框架还必须在高可用、负载均衡、过载保护、通信协议向后兼容、优雅降级、超时处理、无序启动几个维度都做到足够完善才行。

服务发现
Kiev使用zookeeper做服务发现，每个kiev服务开放时会在zookeeper上注册一个节点，包含地址和协议信息。水平扩展时，同质化服务会注册到同一个路径下，产生多个节点。 依赖的服务调用时，从zookeeper上查询当前有哪些节点可以使用，依照负载均衡的策略择一连接并调用。

负载均衡
内置两种负载均衡策略：robin和conhash，并且根据实际业务场景可以定制。

过载保护
Kiev内置了一个过载保护队列，分为10个优先级。每个请求到达时先进入过载保护队列，而后由工作协程(work-coroutine)取出请求进行处理。 如果工作协程的处理速度低于请求到达的速度，过载保护队列就会堆积、甚至堆积满。 当过载保护队列堆满时，新请求到达后会在队列中删除一个更低优先级的请求，腾出一个空位，塞入新请求。 同时，队列中的请求也是有时效性的，过长时间未能被处理的请求会被丢弃掉，以此避免处理已超时的请求。 这种机制保证了当系统过载时尽量将有限的资源提供给关键业务使用。

通信协议向后兼容
由于微服务架构经常需要部分发布，所以选择一个支持向后兼容的通信协议是很必要的一个特性。 Kiev选取protobuf作为通信协议。

与第三方库协同工作
最早期的Kiev是基于异步回调模型的，但是很多第三方库只提供了同步模型的版本，很难搭配使用。 当前的Kiev是CSP并发模型，配合libgo提供的Hook机制，可以将同步模型的第三方库中阻塞等待的CPU时间充分利用起来执行其他逻辑，自动转化成了CSP并发模型；异步回调模型的第三方库也可以使用CSP模型中的Channel来等待回调触发；从而完美地与第三方库协同工作。


我们也更新了kiev中的redis、mysql、fastcgi模块，都改为了协程模型的。

在最初的几个月中，这种方式很好地帮我们提升了开发效率，同时也有着还算不错的性能（Rpc请求差不多有20K左右的QPS）。随着时间的流逝，我们的用户越来越多，请求量也越来越大，终于在某次新品发布后，我们的一个非关键性业务出现了故障。

出现故障的这个业务是一个接受手机端订阅请求的业务，手机端在订阅请求超时后（大概30s），会重新尝试发起请求。由于当时系统过载，处理速度慢于请求速度，大量请求积压在队列中，随着时间的推移，服务处理请求的响应速度越来越慢，最终导致很多请求还没处理完手机端就认为超时了，重新发起了第二次请求，形成雪崩效应。当时紧急增加了一些服务器，恢复了故障，事后总结下来发现，事件的主因还是因为我们没有做好过载保护机制。于是我们决定在Kiev中内置过载保护功能，增加一个分为10个优先级的过载保护队列。每个请求到达时先进入过载保护队列，而后由工作协程(work-coroutine)取出请求进行处理。当过载保护队列堆满时，队列中删除一个最低优先级的请求，腾出一个空位。同时，队列中的请求也是有时效性的，过长时间未能被处理的请求会被丢弃掉，以此避免处理已超时的请求。

随着机器越来越多，以及后续出现了一些超长链路请求的业务形态（这里解释一下长链路请求的问题，长链路请求是指一个请求要流经很多服务处理，在处理流程中，前面的服务一定要等到后面的服务全部处理完成或超时，才会释放其占用的TCP连接，这样的模式会极大地影响整个系统的请求并发数），TCP连接数方面的压力越来越大，最终不得不考虑改为单连接上使用全双工模式。然而当时使用的libco功能过于简单，很难基于此开发全双工模式的RPC框架，恰好当时有一位同事在github上做了一个叫libgo的开源项目，是一个和golang语言一样的CSP并发模型的协程库，于是我们做了一段时间的技术预研，看看能否替换掉现有的libco. 

##C/C++ 协程库boost.coroutine2、魅族libgo、腾讯libco、开源libaco详解
3.2 魅族libgo
     libgo 是一个使用 C++ 编写的协作式调度的stackful协程库, 同时也是一个强大的并行编程库。
      设计之初是为高并发分布式Linux服务端程序开发提供底层框架支持，可以让链接进程序的同步的第三方库变为异步库，不影响逻辑的前提下提升其性能。
目前支持两个平台：
    Linux   (GCC4.8+)
     Windows (Win7、Win8、Win10 x86 and x64 使用VS2013/2015编译)

使用libgo编写并行程序，即可以像golang一样开发迅速且逻辑简洁，又有C++原生的性能优势。

1.提供golang一般功能强大协程，基于corontine编写代码，可以以同步的方式编写简单的代码，同时获得异步的性能

2.支持海量协程, 创建100万个协程只需使用2GB内存

3.允许用户自由控制协程调度点，随时随地变更调度线程数；

4.支持多线程调度协程，极易编写并行代码，高效的并行调度算法，可以有效利用多个CPU核心

5.可以让链接进程序的同步的第三方库变为异步调用，大大提升其性能。再也不用担心某些DB官方不提供异步driver了，比如hiredis、mysqlclient这种客户端驱动可以直接使用，并且可以得到不输于异步driver的性能。

6.动态链接和静态链接全都支持，便于使用C++11的用户静态链接生成可执行文件并部署至低版本的linux系统上。

7.提供协程锁(co_mutex), 定时器, channel等特性, 帮助用户更加容易地编写程序.

8.网络性能强劲，在Linux系统上超越ASIO异步模型；尤其在处理小包和多线程并行方面非常强大

在源码的samples目录下有很多示例代码，内含详细的使用说明，让用户可以很轻易地学会使用libgo。

例子


co_main(int argc, char **argv)
{
    go []{
        printf("1\n");
        co_yield;
        printf("2\n");
    };
    go []{
        printf("3\n");
        co_yield;
        printf("4\n");
    };
    return 0;
}

