
##yield
https://blog.csdn.net/ldw614/article/details/79924587
https://blog.csdn.net/nirendao/article/details/51628693

##协程
https://www.jianshu.com/p/837bb161793a

第2章.协程库的设计与实现

4.框架级
以100%行为模拟的方式HOOK了网络io相关的syscall，可以完全不改代码兼容大多数第三方库；依照专为协程而生的语言的使用经验，提供了协程开发所必须的完整生态；

代表作：libgo

这一层次的协程库，能够100%模拟被hook的syscall的行为，能够兼容任何网络io行为的同步模型的第三方库；由于协程开发生态的完善，对开发人员的要求变得很低，新手也可以写出高效稳定的代码。但由于C++的灵活性，用户行为是不受限的，所以依然存在几个边边角角的难点需要开发者注意：没有gc（开发者要了解协程的调度时机和生命期），TLS的问题，用户不按套路出牌、把逻辑代码run在协程之外，粗粒度的线程锁等等。

第2节.协程栈
我们通常会创建数量非常庞大的协程来支持高并发，协程栈内存占用情况就变成一个不容忽视的问题了；

如果采用线程栈相同的大栈方案（linux系统默认8MB），启动1000个协程就要8GB内存，启动10w个协程就要800GB内存，而每个协程真正使用的栈内存可以几百kb甚至几kb，内存使用率极低，这显然是不可接受的；

如果采用减少协程栈的大小，比如设为128kb，启动1000个协程要128MB内存，启动10w个协程要12.8GB内存，这是一个合理的设置；但是，我们知道有很多人喜欢直接在栈上申请一个64kb的char数组做缓冲区，即使开发者非常小心的不这样奢侈的使用栈内存，也难免第三方库做这样的行为，而只需两层嵌套就会栈溢出了。

栈内存不可太大，也不可太小，这其中是很难权衡的，一旦定死这个值，就只能针对特定的场景，无法做到通用化了； 针对协程栈的内存问题，一般有以下几种方案。

静态栈(Static Stack)

固定大小的栈，存在上述的难以权衡的问题；

但是如果把问题限定在某一个范围，比如说我就只用来写微信后台、并且严格review每一个引入的第三方库的源码，确保其全部谨慎使用栈内存，这种方案也是可以作为实际项目来使用的。

典型代表：libco，它设置了128KB大小的堆栈，15年的时候我们把它引入我们当时的项目中，其后出现过多次栈溢出的问题。

分段栈(Segmented Stack)

gcc提供的“黄金链接器”支持一种允许栈内存不连续的编译参数，实现原理是在每个函数调用开头都插入一段栈内存检测的代码，如果栈内存不够用了就申请一块新的内存，作为栈内存的延续。

这种方案本应是最佳的实现，但如果遇到的第三方库没有使用这种方式来编译(注意:glibc也是这里提到的”第三方库")，那就无法在其中检测栈内存是否需要扩展，栈溢出的风险很大。

拷贝栈(Copy Stack)

每次检测到栈内存不够用时，申请一块更大的新内存，将现有的栈内存copy过去，就像std::vector那样扩展内存。

在某些语言上是可以实现这样的机制，但C++ 是有指针的，栈内存的Copy会导致指向其内存地址的指针失效；又因为其指针的灵活性(可以加减运算)，修改对应的指针成为了一种几乎不可能实现的事情(参照c++ 为什么没办法实现gc原理,详见《C++11新特性解析与应用》第5章 5.2.4节)。

共享栈(Shared Stack)

申请一块大内存作为共享栈(比如：8MB)，每次开始运行协程之前，先把协程栈的内存copy到共享栈中，运行结束后再计算协程栈真正使用的内存，copy出来保存起来，这样每次只需保存真正使用到的栈内存量即可。

这种方案极大程度上避免了内存的浪费，做到了用多少占多少，同等内存条件下，可以启动的协程数量更多，

libco

使用这种方案单机启动了上千万协程。

但是这种方案的缺陷也同样明显：

1.协程切换慢：每次协程切换，都需要2次Copy协程栈内存，这个内存量基本上都在1KB以上，通常是几十kb甚至几百kb，这样的2次Copy要花费很长的时间。

2.栈上引用失效导致隐蔽的bug：例如下面的代码


点击此处添加图片说明文字

​bar这个协程函数里面，启动了一个新的协程，然后bar等待新协程结束后再退出；当切换到新协程时，由于bar协程的栈已经被copy到了其他位置，栈上分配的变量a已经失效，此时调用a.foo就会出现难以预料的结果。

这样的场景在开发中数不胜数，比如：某个处理流程需要聚合多个后端的结果、父协程对子协程做一些计数类的操作等等等等

有人说我可以把变量a分配到堆上，这样的改法确实可以解决这个已经发现的bug；那其他没发现的怎么办呢，难道每个变量都放到堆上以提前规避这个坑？这显然是不切实际的。

早期的libgo也使用过共享栈的方式，也正是因为作者在实际开发中遇到了这样的问题，才放弃了共享栈的方式。

虚拟内存栈(Virtual Memory Stack)

既然前面提到的4种协程栈都有这样那样的弊端，那么有没有一种方案能够相对完美的解决这个问题？答案就是虚拟内存栈。

Linux、Windows、MacOS三大主流操作系统都有这样一个虚拟内存机制：进程申请的内存并不会立即被映射成物理内存，而是仅管理于虚拟内存中，真正对其读写时会触发缺页中断，此时才会映射为物理内存。

比如：我在进程中malloc了1MB的内存，但是不做读写，那么物理内存占用是不会增加的；当我读写这块内存的第一个字节时，系统才会将这1MB内存中的第一页(默认页大小4KB)映射为物理内存，此时物理内存的占用会增加4KB，以此类推，可以做到用多少占多少，冗余不超过一个内存页大小。

基于这样一个机制，libgo为每个协程malloc 1MB的虚拟内存作为协程栈(这个值是可以定制化的)；不做读写操作就不会占用物理内存，协程栈使用了多少才会占用多少物理内存，实现了与共享栈近似的内存使用率，并且不存在共享栈的两大弊端。

典型代表：

libgo

第3节.协程调度
像操作系统的进程调度一样，协程调度也有多种方案可选，也有公平调度和不公平调度之分。

栈式调度

栈式调度是典型的不公平调度：协程队列是一个栈式的结构，每次创建的协程都置于栈顶，并且会立即暂停当前协程并切换至子协程中运行，子协程运行结束(或其他原因导致切换出来)后，继续切换回来执行父协程；越是处于栈底部的协程(越早创建的协程)，被调度到的机会越少；

甚至某些场景下会产生隐晦的死循环导致永远在栈顶的两个协程间切来切去，其他协程全部无法执行。

典型代表：

libco

星切调度(非对称协程调度)

调度线程 -> 协程A -> 调度线程 -> 协程B -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来就像是星星一样，因此戏称为星切。

将当前可调度的协程组织成先进先出的队列(runnable list)，顺序pop出来做调度；新创建的协程排入队尾，调度一次后如果状态依然是可调度(runnable)的协程则排入队尾，调度一次后如果状态变为阻塞，那阻塞事件触发后也一样排入队尾，是为公平调度。

典型代表：

libgo

环切调度(对称协程调度)

调度线程 -> 协程A -> 协程B -> 协程C -> 协程D -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来呈环状，因此戏称为环切。

从调度顺序上可以发现，环切的切换次数仅为星切的一半，可以带来更高的整体切换速度；但是多线程调度、WorkSteal方面会带来一定的挑战。

这种方案也是libgo后续优化的一个方向

多线程调度、负载均衡与WorkSteal

本节的内容其实不是协程库的必选项，互联网服务端开发领域现在主流方案都是微服务，单线程多进程的模型不会有额外的负担。

但是某些场景下多进程会有很昂贵的额外成本(比如：开发一个数据库)，只能用多线程来解决，libgo为了有更广阔的适用性，实现了多线程调度和Worksteal。同时也突破了传统协程库仅用来处理网络io密集型业务的局限，也能适用于cpu密集型业务，充当并行编程库来使用。

libgo的多线程调度采用N:M模型，调度线程数量可以动态增加，但不能减少； 每个调度线程持有一个Processer(后文简称: P)，每个P持有3个runnable协程队列(普通队列、IO触发队列、亲缘性队列)，其中普通队列保存的是可以被偷取的协程；当某个P空闲时，会去其他P的队列尾部偷取一些协程过来执行，以此实现负载均衡。

为了IO方面降低线程竞争，libgo会为每个调度线程在必要的时候单独创建一个epoll；

关于每个epoll的使用，会在后面的本章第4节.HOOK-网络io中展开详细论述；其他关于多线程的设计会贯穿全文的逐个介绍。

​定时器

libgo框架的主调度器提供了一个基于红黑树的定时器，会在调度线程的主循环中被执行，这样的设计可以与epoll更好地协同工作，无论是定时器还是epoll监听的fd都可以最及时的触发。

使用co_timer_add接口可以添加一个定时任务，co_timer_add接口接受两个参数，第一个参数是可以是std::chrono::system_clock::time_point，也可以是std::chrono::steady_clock::time_point，还可以是std::chrono库里的一个duration。第二个参数接受一个回调函数，可以是函数指针、仿函数、lambda等等；

当第一个参数使用system_clock::time_point时，表示定时任务跟随系统时间的变化而变化，可以通过调整操作系统的时间设置提前或延缓定时任务的执行。

当第一个参数使用另外两种类型时，定时任务不随系统时间的变化而变化。

co_timer_add接口返回一个co::TimerId类型的定时任务id，可以用来取消定时任务。

取消定时任务有种方式：co_timer_cancel和co_timer_block_cancel，均会返回一个bool类型表示是否取消成功。

使用co_timer_cancel，会立即返回，即使定时任务正在被执行。

使用co_timer_block_cancel，如果定时任务正在被执行，则会阻塞地等待任务完成后返回false；否则会立即返回；

需要注意的是co_timer_block_cancel的阻塞行为是使用自旋锁实现的，如果定时任务耗时较长，co_timer_block_cancel的阻塞行为不但会阻塞当前调度线程，还会产生高昂的cpu开销；这个接口是设计用来在libgo内部使用的，请用户谨慎使用！

CLS(Coroutine Local Storage)(协程本地存储)

CLS类似于TLS(Thread Local Storage)；

这个功能是HOOK DNS函数族的基石，没有CLS的协程库是无法HOOK DNS函数族的。

libgo

提供了一个行为是TLS超集的CLS功能，CLS变量可以定义在全局作用域、块作用域(函数体内)、类的静态成员，除此TLS也支持的这三种场景外，还可以作为类的非静态成员。

注：

libco

也有CLS功能，但是仅支持全局作用域

CLS的使用方式参见tutorail文件夹下的sample13_cls.cpp教程代码。

线程池

除了前文提到的各种边角问题之外，还有一个非常常见的边角问题：文件IO 笔者曾经努力尝试过HOOK文件IO操作，但很不幸linux系统中，文件fd是无法使用poll、select、epoll正确监听可读可写状态的；linux提供的异步文件IO系统调用nio又不支持操作系统的文件缓存，不适合用来实现HOOK(这会导致用户的所有文件IO都不经过系统缓存而直接操作硬盘，这是一种不恰当的做法)。

除此之外也还会有其他不能HOOK或未被HOOK的阻塞syscall，因此需要一个线程池机制来解决这种阻塞行为对协程调度的干扰。

libgo提供了一个宏：co_await，来辅助用户完成线程池与协程的交互。


​在协程中使用


​可以把func投递到线程池中，并且挂起当前协程，直到func完成后协程会被唤醒，继续执行下去。 也可以使用


​等待bar在线程池中完成，并将bar的返回值写入变量a中。 co_await也同样可以在协程之外被调用。

另外，为了用户更灵活的定制线程数量，也为了libgo不偷起后台线程的操守；线程池并不会自行启动，需要用户自行启动一个或多个线程执行co_sched.GetThreadPool().RunLoop();

调试

libgo作为框架级的协程库，调试机制是必不可少的。

1.可以设置co_sched.GetOptions().debug打印一些log，具体flag见config.h

2.可以设置一个协程事件监听器，详见tutorial文件夹下的sample12_listener.cpp教程代码

3.编译时添加cmake参数：-DENABLE_DEBUGGER=ON 开启debug信息收集后，可以使用co::CoDebugger类获取一些调试信息，详见debugger.h的注释

4.后续还会提供更多调试手段

协程之外(运行在线程上的代码)

前文提到了很多功能都可以在线程上执行：Channel、co_await、co_mutex、定时器、CLS

跨平台

libgo支持三大主流系统：linux、windows、mac-os

linux是主打平台，也是libgo运行性能最好的平台，master分支永远支持linux

win分支支持windows系统，会不定期的将master分支的新功能合入其中

mac的情况同windows

（个人开发者精力有限，还请见谅！）

上层封装

笔者另有一个开源库：libgonet，是基于libgo封装的linux协程网络库，使用起来极为方便。

如果你要开发一个网络服务或rpc框架，更推荐从libgonet写起，毕竟即使有协程，socket相关的处理也并不轻松。

未来的发展方向

1.目前是使用go、go_stack、go_dispatch三个不同的宏来设置协程的属性，这种方式不够灵活，后续要改成： go stack(1024 * 1024) dispatch(::co::egod_robin) func; 这样的语法形式，可以更灵活的定制协程属性。

2.基于(1)的新语法，实现“协程亲缘性”功能，将协程绑定到指定线程上，并防止被steal。

3.优化协程切换速度：

A）使用环切调度替代现在的星切调度(CoYeild时选择下一个切换目标)，必要时才切换回线程处理epoll、定时器、sleep等逻辑，同时协调好多线程调度

B）调度器的Run函数里面做了很多协程切换之外的事情，尽量降低这部分在非必要时的cpu消耗，比如：有任务加入定时器是设置一个tls标记为true，只有标记为true时才去处理定时器相关逻辑。

C）调度器中的runnable队列使用了自旋锁，没有竞争时对原子变量的操作也是比较昂贵的，runnable队列可以优化成多写一读，仅在写入端加锁的队列。

4.协程对象Task内存布局调优，tls池化，每个池使用多写一读链表队列，申请时仅在当前线程的池中申请，可以免锁，释放时均衡每个线程的池水水位，可以塞入其他线程的池中。

5.libgo之外，会进一步寻找和当前已经比较成熟的非协程的开发框架的结合方案，让还未能用上协程的用户低成本的用上协程。

libgo开源地址:https://github.com/yyzybb537/libgo

https://www.cnblogs.com/sniperHW/archive/2012/08/05/2624334.html
https://www.cnblogs.com/heluan/p/9689751.html

##context
寄存器
https://www.jianshu.com/p/57128e477efb
make_fcountext、jump_fcontext
https://segmentfault.com/a/1190000019154852

协程分析之context上下文切换
https://blog.csdn.net/waruqi/article/details/53201416
https://www.liangzl.com/get-article-detail-578.html
https://blog.csdn.net/libaineu2004/article/details/80554870
https://blog.csdn.net/zdyueguanyun/article/details/60782260
https://gitee.com/DreamThat/libgo
https://www.jianshu.com/p/43216b429583

##阻塞非阻塞 异步同步
http://www.sohu.com/a/231730171_231667

###Unix网络编程中的五种IO模型

Blocking IO - 阻塞IO
NoneBlocking IO - 非阻塞IO
IO multiplexing - IO多路复用
signal driven IO - 信号驱动IO
asynchronous IO - 异步IO

由于signal driven IO在实际使用中并不常用，所以这里只讨论剩下的四种IO模型。

在讨论之前先说明一下IO发生时涉及到的对象和步骤，对于一个network IO，它会涉及到两个系统对象：

application 调用这个IO的进程
kernel 系统内核
那他们经历的两个交互过程是：

阶段1 wait for data 等待数据准备
阶段2 copy data from kernel to user 将数据从内核拷贝到用户进程中
之所以会有同步、异步、阻塞和非阻塞这几种说法就是根据程序在这两个阶段的处理方式不同而产生的。了解了这些背景之后，我们就分别针对四种IO模型进行讲解

###Blocking IO - 阻塞IO

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概如下图：

当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。对于network IO来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

所以，blocking IO的特点就是在IO执行的两个阶段都被block了。

###NoneBlockingIO - 非阻塞IO

linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：


从图中可以看出，当用户进程发出recvfrom这个系统调用后，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个结果（no datagram ready）。从用户进程角度讲 ，它发起一个操作后，并没有等待，而是马上就得到了一个结果。用户进程得知数据还没有准备好后，它可以每隔一段时间再次发送recvfrom操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

所以，用户进程其实是需要不断的主动询问kernel数据好了没有。

###IO multiplexing - IO多路复用
I/O多路复用(multiplexing)是网络编程中最常用的模型，像我们最常用的select、epoll都属于这种模型。以select为例：

看起来它与blocking I/O很相似，两个阶段都阻塞。但它与blocking I/O的一个重要区别就是它可以等待多个数据报就绪（datagram ready），即可以处理多个连接。这里的select相当于一个“代理”，调用select以后进程会被select阻塞，这时候在内核空间内select会监听指定的多个datagram (如socket连接)，如果其中任意一个数据就绪了就返回。此时程序再进行数据读取操作，将数据拷贝至当前进程内。由于select可以监听多个socket，我们可以用它来处理多个连接。

在select模型中每个socket一般都设置成non-blocking，虽然等待数据阶段仍然是阻塞状态，但是它是被select调用阻塞的，而不是直接被I/O阻塞的。select底层通过轮询机制来判断每个socket读写是否就绪。

当然select也有一些缺点，比如底层轮询机制会增加开销、支持的文件描述符数量过少等。为此，Linux引入了epoll作为select的改进版本。

###asynchronous IO - 异步IO

异步I/O在网络编程中几乎用不到，在File I/O中可能会用到：

这里面的读取操作的语义与上面的几种模型都不同。这里的读取操作(aio_read)会通知内核进行读取操作并将数据拷贝至进程中，完事后通知进程整个操作全部完成（绑定一个回调函数处理数据）。读取操作会立刻返回，程序可以进行其它的操作，所有的读取、拷贝工作都由内核去做，做完以后通知进程，进程调用绑定的回调函数来处理数据。

https://blog.csdn.net/zh13544539220/article/details/44856363

##HTTP性能极限优化
https://blog.csdn.net/russell_tao/article/details/103952639#comments

##详解HTTP2四大核心特性
https://developer.51cto.com/art/201910/604255.htm

#libgo 
##libgo github
https://github.com/yyzybb537/libgo/blob/master/tutorial/sample14_defer.cpp

##libgo libco 魅族架构
Kiev框架简介
kiev是魅族科技推送平台目前使用的Linux-C++后台开发框架。从2012年立项起，先后由多位魅族资深架构师、资深C++工程师倾力打造，到本文写就的时间为止，已经在推送平台这个千万用户级的大型分布式系统上经历了近5年的考验。如今Kiev在魅族推送平台中，每天为上百个服务完成数百亿次RPC调用。

kiev作为一套完整的开发框架，是专为大型分布式系统后台打造的C++开发框架，由以下几个组件组成：

RPC框架(TCP/UDP)
FastCGI框架
redis客户端(基于hiredis封装)
mysql客户端(基于mysqlclient封装)
mongodb客户端
配置中心客户端(Http协议, 基于curl实现)
基于zookeeper的分布式组件(服务发现、负载均衡)
日志模块
状态监控模块
核心模块是一个开源的`CSP并发模型`协程库(libgo)
并发模型
Kiev采用了很先进的CSP开发模型的一个变种(golang就是这种模型)，这一模型是继承自libgo的。 选择这种模型的主要原因是这种模型的开发效率远高于异步回调模型，同时不需要在性能上做出任何妥协，在文中会对常见的几种模型做详细的对比。

CSP模型
CSP(Communicating Sequential Process)模型是一种目前非常流行的并发模型，golang语言所采用的并发模型就是CSP模型。 在CSP模型中，协程与协程间不直接通信，也不像Actor模型那样直接向目标协程投递信息，而是通过一个Channel来交换数据。

RPC框架
RPC（Remote Procedure Call）是一种远程调用协议，简单地说就是能使应用像调用本地方法一样的调用远程的过程或服务，可以应用在分布式服务、分布式计算、远程服务调用等许多场景。说起 RPC 大家并不陌生，业界有很多开源的优秀 RPC 框架，例如 Dubbo、Thrift、gRPC、Hprose 等等。 RPC框架的出现是为了简化后台内部各服务间的网络通讯，让开发人员可以专注于业务逻辑，而不必与复杂的网络通讯打交道。 在我们看来，RPC框架绝不仅仅是封装一下网络通讯就可以了的，要想应对数以百计的不同服务、数千万用户、百亿级PV的业务量挑战，RPC框架还必须在高可用、负载均衡、过载保护、通信协议向后兼容、优雅降级、超时处理、无序启动几个维度都做到足够完善才行。

服务发现
Kiev使用zookeeper做服务发现，每个kiev服务开放时会在zookeeper上注册一个节点，包含地址和协议信息。水平扩展时，同质化服务会注册到同一个路径下，产生多个节点。 依赖的服务调用时，从zookeeper上查询当前有哪些节点可以使用，依照负载均衡的策略择一连接并调用。

负载均衡
内置两种负载均衡策略：robin和conhash，并且根据实际业务场景可以定制。

过载保护
Kiev内置了一个过载保护队列，分为10个优先级。每个请求到达时先进入过载保护队列，而后由工作协程(work-coroutine)取出请求进行处理。 如果工作协程的处理速度低于请求到达的速度，过载保护队列就会堆积、甚至堆积满。 当过载保护队列堆满时，新请求到达后会在队列中删除一个更低优先级的请求，腾出一个空位，塞入新请求。 同时，队列中的请求也是有时效性的，过长时间未能被处理的请求会被丢弃掉，以此避免处理已超时的请求。 这种机制保证了当系统过载时尽量将有限的资源提供给关键业务使用。

通信协议向后兼容
由于微服务架构经常需要部分发布，所以选择一个支持向后兼容的通信协议是很必要的一个特性。 Kiev选取protobuf作为通信协议。

与第三方库协同工作
最早期的Kiev是基于异步回调模型的，但是很多第三方库只提供了同步模型的版本，很难搭配使用。 当前的Kiev是CSP并发模型，配合libgo提供的Hook机制，可以将同步模型的第三方库中阻塞等待的CPU时间充分利用起来执行其他逻辑，自动转化成了CSP并发模型；异步回调模型的第三方库也可以使用CSP模型中的Channel来等待回调触发；从而完美地与第三方库协同工作。


我们也更新了kiev中的redis、mysql、fastcgi模块，都改为了协程模型的。

在最初的几个月中，这种方式很好地帮我们提升了开发效率，同时也有着还算不错的性能（Rpc请求差不多有20K左右的QPS）。随着时间的流逝，我们的用户越来越多，请求量也越来越大，终于在某次新品发布后，我们的一个非关键性业务出现了故障。

出现故障的这个业务是一个接受手机端订阅请求的业务，手机端在订阅请求超时后（大概30s），会重新尝试发起请求。由于当时系统过载，处理速度慢于请求速度，大量请求积压在队列中，随着时间的推移，服务处理请求的响应速度越来越慢，最终导致很多请求还没处理完手机端就认为超时了，重新发起了第二次请求，形成雪崩效应。当时紧急增加了一些服务器，恢复了故障，事后总结下来发现，事件的主因还是因为我们没有做好过载保护机制。于是我们决定在Kiev中内置过载保护功能，增加一个分为10个优先级的过载保护队列。每个请求到达时先进入过载保护队列，而后由工作协程(work-coroutine)取出请求进行处理。当过载保护队列堆满时，队列中删除一个最低优先级的请求，腾出一个空位。同时，队列中的请求也是有时效性的，过长时间未能被处理的请求会被丢弃掉，以此避免处理已超时的请求。

随着机器越来越多，以及后续出现了一些超长链路请求的业务形态（这里解释一下长链路请求的问题，长链路请求是指一个请求要流经很多服务处理，在处理流程中，前面的服务一定要等到后面的服务全部处理完成或超时，才会释放其占用的TCP连接，这样的模式会极大地影响整个系统的请求并发数），TCP连接数方面的压力越来越大，最终不得不考虑改为单连接上使用全双工模式。然而当时使用的libco功能过于简单，很难基于此开发全双工模式的RPC框架，恰好当时有一位同事在github上做了一个叫libgo的开源项目，是一个和golang语言一样的CSP并发模型的协程库，于是我们做了一段时间的技术预研，看看能否替换掉现有的libco. 

##C/C++ 协程库boost.coroutine2、魅族libgo、腾讯libco、开源libaco详解
3.2 魅族libgo
     libgo 是一个使用 C++ 编写的协作式调度的stackful协程库, 同时也是一个强大的并行编程库。
      设计之初是为高并发分布式Linux服务端程序开发提供底层框架支持，可以让链接进程序的同步的第三方库变为异步库，不影响逻辑的前提下提升其性能。
目前支持两个平台：
    Linux   (GCC4.8+)
     Windows (Win7、Win8、Win10 x86 and x64 使用VS2013/2015编译)

使用libgo编写并行程序，即可以像golang一样开发迅速且逻辑简洁，又有C++原生的性能优势。

1.提供golang一般功能强大协程，基于corontine编写代码，可以以同步的方式编写简单的代码，同时获得异步的性能

2.支持海量协程, 创建100万个协程只需使用2GB内存

3.允许用户自由控制协程调度点，随时随地变更调度线程数；

4.支持多线程调度协程，极易编写并行代码，高效的并行调度算法，可以有效利用多个CPU核心

5.可以让链接进程序的同步的第三方库变为异步调用，大大提升其性能。再也不用担心某些DB官方不提供异步driver了，比如hiredis、mysqlclient这种客户端驱动可以直接使用，并且可以得到不输于异步driver的性能。

6.动态链接和静态链接全都支持，便于使用C++11的用户静态链接生成可执行文件并部署至低版本的linux系统上。

7.提供协程锁(co_mutex), 定时器, channel等特性, 帮助用户更加容易地编写程序.

8.网络性能强劲，在Linux系统上超越ASIO异步模型；尤其在处理小包和多线程并行方面非常强大

在源码的samples目录下有很多示例代码，内含详细的使用说明，让用户可以很轻易地学会使用libgo。

例子


co_main(int argc, char **argv)
{
    go []{
        printf("1\n");
        co_yield;
        printf("2\n");
    };
    go []{
        printf("3\n");
        co_yield;
        printf("4\n");
    };
    return 0;
}

