#ZAB
##ZAB协议
https://cloud.tencent.com/developer/article/1469528
ZAB协议是为分布式协调服务Zookeeper专门设计的一种支持崩溃恢复的原子广播协议。在Zookeeper中，主要依赖ZAB协议来实现分布式数据一致性。

ZAB协议内容
ZAB协议主要有两种模式：崩溃恢复和消息广播。

当服务器启动、或者Leader宕机、或者Leader与绝大多数Follower无法正常通信时，ZAB协议就会进入崩溃恢复模式用来产生新的Leader。

当选举产生新的Leader并且完成数据同步以后，ZAB协议将由崩溃恢复模式转变为消息广播模式。

在ZAB协议的设计中，每一个进程都可能处于下面三种状态中的一种：

LOOKING：Leader选举阶段
FOLLOWING：Follower服务器和Leader保持同步状态
LEADING：Leader服务器作为主进程领导状态
ZAB协议需要保证以下两条原则：

确保那些已经在Leader服务器上提交的事务最终被所有的服务器进行提交(在Leader上提交，说明绝大多数Follower已经接收到了事务的Proposal请求并返回了ACK响应，只是还未收到commit请求)
确保丢弃那些只在Leader服务器上被提出的事务(只在Leader上被提出说明没有其他Follower并没有收到Proposal请求)
消息广播
消息广播模式是在整个集群稳定运行时的模式。它的操作类似于两阶段提交。

在消息的广播过程中，Leader会为每一个Follower准备一个事务队列，该队列符合FIFO原则。假设Follower接收到了客户端的写请求，写请求会被转发至Leader处理，当Leader收到客户端的写请求时，主要有以下步骤：

为写请求的事务Proposal分配一个全局唯一递增的ID(ZXID)
将这个事务放入每个Follower对应的事务队列，并按照FIFO顺序进行广播
Follower接收到事务请求后，将该事务请求写入本地事务日志文件，并在写成功后给Leader返回ACK响应
当Leader收到过半的Follower返回的ACK，便向所有的Follower发送commit请求通知Follower执行事务提交，同时Leader自身也完成事务提交
Follower收到commit请求后，完成事务提交
Leader为每个Follower使用队列做了异步解耦，大大降低同步阻塞，提高了系统的吞吐量。

崩溃恢复
崩溃恢复主要用来当Leader宕机或者Leader与大多数Follower因为网络原因无法通信时进行新Leader的选举或者集群启动时进行Leader的选举。崩溃恢复主要由两个过程组成：Leader选举、数据同步。

Leader选举
保证上述原则实现Leader很简单，只要保证新选出来的Leader服务器拥有最大的ZXID就可以，那么这个新Leader一定具有所有已提交的事务，还可以省去检查Proposal的提交和丢弃工作。

首先确认一个点，每个Zookeeper节点进入LOOKING状态时，都会发起选举流程，其他的Zookeeper机器收到该请求时，只有两种响应：

接收选票提议，同意Zookeeper节点成为Leader候选人
否决选票提议，并推荐自己上一次推荐的服务器作为Leader候选人
其次我们来讲述一下ZXID这个概念，ZXID其实就是一个全局单调递增的唯一的事务ID，由高32位的epoch表示选举周期和低32位的自增事务ID组成，每经历一次选举产生新的Leader，epoch的值将加1，而且低32位的的ID将被置为0，重新从0开始自增。

下面简单描述一下准Leader选举的过程，后面会出一篇源码分析来详解Leader的选举：

首先参与Leader选举的服务器必须是状态位LOOKING状态的节点
Zookeeper节点向其他的服务器节点发送自己要成为Leader候选人的请求(请求包含ZXID)
其他节点收到请求后，将本地事务日志的ZXID与请求中ZXID进行比较，如果发现比自己的大(如果ZXID一样大就比较myid(这个后面讲))，就同意该节点成为候选人并更新 该节点为推荐候选人而不是自己然后通知其他的节点，否则还是将自己作为候选人推荐
每次投票都会进行统计，判断是否有过半的服务器收到的推荐候选人是一致的，如果过半就认为已经选出了准Leader
一旦选举完成，就需要改变服务状态，新的Leader置为LEADING状态，其他机器转变为FOLLOWING状态
数据同步
只有当集群中的过半及其完成了数据同步，准Leader就可以真正的成为Leader。Follower只会接收ZXID比自己的最后一次事务的ZXID大的提议。

所有的Follower向准Leader发送自己的最后接收事务的epoch
准Leader选出最大的epoch，并在此基础上进行加1，然后将新的epoch发送给所有的Follower
Follower收到新的epoch之后，与自己的进行比较，小于就将自己的epoch更新成新的epoch，并向准Leader反馈ACK信息(epoch、历史事务集合)
准Leader收到ACK消息后，会在所有历史事务集合中选出其中一个历史事务集合作为初始化历史事务集合，该事务集合必须满足最大ZXID
准Leader将epoch和初始化历史事务集合发送给过半的Follower，每个Follower分配一个事务队列然后逐条将事务发送给Follower
Follower接收到事务请求后，如果已执行过则跳过，未执行则执行事务并反馈响应给准Leader
准Leader收到响应后则发起事务commit请求，提交事务
数据完成同步后，准Leader就是Leader，ZAB协议由崩溃恢复模式进入消息广播模式
ZAB和Paxos区别
本质区别在于设计的目的不一样，ZAB协议主要使用来构建一个高可用的分布式数据主备系统，Paxos算法主要是用来解决数据一致性。


##ZAB协议
　　介绍
　　1、zab协议是为分布式协调服务zookpeer专门设计的一种支持崩溃恢复的原子广播协议

　　2、在zookeeper中主要依赖ZAB协议来实现数据一致性，基于该协议zk实现了一种主备模式的系统架构来保证集群中各个副本之间数据的一致性。具体就是zk使用一个单一的主进程来接收并处理客户端的事务请求（就是写请求），并采用ZAB的原子广播协议，将服务器数据的状态变更以事务proposal的形式广播到所有的副本进程上去。

 

　　事务请求的处理方式
　　所有的事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则是Follow服务器。Leader负责将一个事务请求转换成一个事务proposal，并将该proposal分发给集群中所有的Follow，之后Leader需要等待所有的Follow的反馈，一单超过半数Follow进行了正确的反馈后，那么Leader就会再次向所有的Follow发送commit消息，要求其将前一个proposal进行提交

　　注意：如果集群中非Leader服务器接收到事务请求，这些非Leader服务器会将事务请求转发给Leader服务器，只有Leader才能处理事务请求。

　　

　　协议具体内容
　　ZAB协议包括两种基本模式：分别是崩溃恢复和消息广播

　　当整个集群启动过程中或者当Leader服务器出现网络中断，崩溃退出或重启等异常时，ZAB协议j就会进入恢复模式并选举产生新的Leader，当选举产生了新的Leader，同时集群中有过半的机器与该Leader服务器完成了状态同步（即数据同步）之后，ZAB协议就会退出崩溃恢复模式，进入消息广播模式。这时如果有一台遵守ZAB协议的服务器加入集群，因为此时集群中已经存在一个Leader服务器在广播消息，那么新加入的服务器自觉的进入恢复模式：找到Leader服务器并与之完成数据同步然后一起参与到消息广播流程中去。

　　当Leader出现崩溃退出或者机器重启，亦或是集群中已经不存在过半的服务器与Leader保持正常的通信，ZAB就会重新发一轮Leader选举并实现数据同步，最后又进入消息广播模式，接收事务请求

　　消息必须是有顺序的
　　在整个消息广播过程中，Leader会将每个事务请求转换成对应的proposal来进行广播，并且在广播事务proposal之前，Leader服务器会首先为这个事务proposal分配一个全局的单调递增的唯一ID，称之为事务ID（即ZXID），由于ZAB需要保证每一个消息严格的因果关系，因此必须将每一个proposal按照其ZXID的先后顺序来进行排序与处理。

　　消息广播
　　ZAB协议的消息广播使用的是一个原子广播协议，类似于二阶段提交，Leader接收事务请求，并转换成proposal广播给其他的Follow，然后过半的Follow ack消息，最后再给广播commit消息完成事务提交。

　　具体的，在消息广播过程中，Leader为每个Follow服务器分配一个单独的队列，然后将需要广播的proposal依次放到队列中去，并且根据FIFO策略进行消息发送。每一个Follow接收到proposal后，都会首先将其以事务日志的形式写入本地磁盘中，并且写入成功后反馈Leader一个ACK响应。当Leader接收到超过半数的ACK响应后，就会广播一个commit消息给Follow已通知他们完成事务提交，同时Leader自身也会完成事务的提交。

 

　　崩溃恢复
　　Leader挂了之后，ZAB协议就自动进入崩溃恢复模式，选举出新的Leader，并完成数据同步，然后退出崩溃恢复模式进入消息广播模式

 

　　ZAB协议如何保证数据一致性
　　异常情况：

　　1、假设一个事务在Leader上提交了，并且过半Follow都响应ACK了，但是Leader将commit消息发出后就挂了

　　2、假设一个事务在Leader提交了之后，Leader就挂掉了。

　　要保证如果发生上述2种情况，数据还能保持一致性，那么ZAB协议选举算法必须确保已经提交的proposal（发送过commit消息），在Follow上也必须完成提交；并且丢弃已经被跳过的事务proposal。

通过ZAB协议选举算法选举出来的Leader必须是拥有集群中最高编号（ZXID）proposal的机器，拥有最高编号说明新Leader一定具有所有已提交的提案。更为重要的是，如果让具有最高编号的事务proposal的机器成为Leader，就可以省去Leader服务器检查proposal的提交和丢弃工作的这一步操作了。

　　ZAB是如何数据同步？
　　完成Leader选举后（新Leader具有最高编号），在正式开始工作之前（接收事务请求，然后提出新的proposal）,Leader服务器会首先确认事务日志中的所有的Proposal是否都已经被集群中过半的提交了。

　　Leader服务器需要确保所有的Follow服务器能够接收到每一条事务proposal, 并且能将所有已经提交的事务proposal应用到内存数据库中去。等到Follow将所有尚未同步的事务proposal都从Leader服务器上同步过来并应用到内存数据库中去，Leader才会把该Follow加入到真正可用的Follow列表中。

　　ZAB是如何处理需要丢弃的Proposal？
　　在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的数字，其中低32位可以看作成一个简单的单调递增计数器了，针对客户端每一个事务请求，Leader在产生新的事务Proposal时，都会对该计数器加1，而高32位则代表了Leader周期的epoch编号（可以理解为选举的届期），每当选举产生一个新的Leader，就会从这个Leader服务器上取出本地事务日志中最大的编号proposal的ZXID，并从ZXID中解析得到对应epoch编号，然后再对其进行加1，之后就以此编号作为新的epoch值，并将地32位置为0开始生成新的ZXID，ZAB协议通过epoch编号来区分Leader变化周期，能够有效的避免了不同的Leader错误的使用了相同的ZXID编号提出了不一样的proposal的异常情况。

　　基于这样的策略，当一个包含了上一个Leader周期中尚未提交过的事务proposal的服务器启动时，当这台机器加入集群中，以Follow角色连接上Leader服务器后，Leader服务器会根据自己服务器上最后提交的proposal来和Follow服务器的proposal进行比对，比对的结果肯定是Leader要求Follow进行一个回退操作，回退到一个确实已经被集群中过半机器提交的最新proposal。

　　ZAB协议特性：
　　1、ZAB协议需要确保那些已经在Leader服务器上提交（commit）的事务最终被所有的服务器提交

　　2、ZAB协议需要确保丢弃那些只在Leader上被提出的事务(只是被提出还没有被提交)

##阿里巴巴为什么不用 ZooKeeper 做服务发现？
https://www.jianshu.com/p/7fafac39bf84

有多少人想到过或者思考过一个问题：服务发现，ZooKeeper 真的是最佳选择么？而回望历史，我们也偶有迷思，在服务发现这个场景下，如果当年 ZooKeeper 的诞生之日比我们 HSF 的注册中心 ConfigServer 早一点会怎样？我们会不会走向先使用 ZooKeeper 然后疯狂改造与修补 ZooKeeper 以适应阿里巴巴的服务化场景与需求的弯路？但是，站在今天和前人的肩膀上，我们从未如今天这样坚定的认知到，在服务发现领域，ZooKeeper 根本就不能算是最佳的选择，一如这些年一直与我们同行的 Eureka 以及这篇文章 《Eureka! Why You Shouldn’t Use ZooKeeper for Service Discovery》那坚定的阐述一样，为什么你不应该用 ZooKeeper 做服务发现！吾道不孤矣。注册中心需求分析及关键设计考量接下来，让我们回归对服务发现的需求分析，结合阿里巴巴在关键场景上的实践，来一一分析，一起探讨为何说 ZooKeeper 并不是最合适的注册中心解决方案。注册中心是 CP 还是 AP 系统?CAP 和 BASE 理论相信读者都已经耳熟能详，其业已成了指导分布式系统及互联网应用构建的关键原则之一，在此不再赘述其理论，我们直接进入对注册中心的数据一致性和可用性需求的分析:

数据一致性需求分析
注册中心最本质的功能可以看成是一个 Query 函数 Si = F(service-name)，以 service-name 为查询参数，service-name 对应的服务的可用的 endpoints (ip:port)列表为返回值.

注: 后文将 service 简写为 svc。

先来看看关键数据 endpoints (ip:port) 不一致性带来的影响，即 CAP 中的 C 不满足带来的后果 :

如上图所示，如果一个 svcB 部署了 10 个节点 (副本 /Replica），如果对于同一个服务名 svcB, 调用者 svcA 的 2 个节点的 2 次查询返回了不一致的数据，例如: S1 = { ip1,ip2,ip3...,ip9 }, S2 = { ip2,ip3,....ip10 }, 那么这次不一致带来的影响是什么？相信你一定已经看出来了，svcB 的各个节点流量会有一点不均衡。ip1 和 ip10 相对其它 8 个节点{ip2...ip9}，请求流量小了一点，但很明显，在分布式系统中，即使是对等部署的服务，因为请求到达的时间，硬件的状态，操作系统的调度，虚拟机的 GC 等，任何一个时间点，这些对等部署的节点状态也不可能完全一致，而流量不一致的情况下，只要注册中心在 SLA 承诺的时间内（例如 1s 内）将数据收敛到一致状态（即满足最终一致），流量将很快趋于统计学意义上的一致，所以注册中心以最终一致的模型设计在生产实践中完全可以接受。

通过以上我们的阐述可以看到，在 CAP 的权衡中，注册中心的可用性比数据强一致性更宝贵，所以整体设计更应该偏向 AP，而非 CP，数据不一致在可接受范围，而 P 下舍弃 A 却完全违反了注册中心不能因为自身的任何原因破坏服务本身的可连通性的原则。

我们知道 ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，以及宕机之后的数据可恢复，这是非常好的特性，但是我们要问，在服务发现场景中，其最核心的数据 - 实时的健康的服务的地址列表真的需要数据持久化么？对于这份数据，答案是否定的。


通过事务日志，持久化连续记录这个变化过程其实意义不大，因为在服务发现中，服务调用发起方更关注的是其要调用的服务的实时的地址列表和实时健康状态，每次发起调用时，并不关心要调用的服务的历史服务地址列表、过去的健康状态。但是为什么又说需要呢，因为一个完整的生产可用的注册中心，除了服务的实时地址列表以及实时的健康状态之外，还会存储一些服务的元数据信息，例如服务的版本，分组，所在的数据中心，权重，鉴权策略信息，service label 等元信息，这些数据需要持久化存储，并且注册中心应该提供对这些元信息的检索的能力。Service Health Check使用 ZooKeeper 作为服务注册中心时，服务的健康检测常利用 ZooKeeper 的 Session 活性 Track 机制 以及结合 Ephemeral ZNode 的机制，简单而言，就是将服务的健康监测绑定在了 ZooKeeper 对于 Session 的健康监测上，或者说绑定在 TCP 长链接活性探测上了。这在很多时候也会造成致命的问题，ZK 与服务提供者机器之间的 TCP 长链接活性探测正常的时候，该服务就是健康的么？答案当然是否定的！注册中心应该提供更丰富的健康监测方案，服务的健康与否的逻辑应该开放给服务提供方自己定义，而不是一刀切搞成了 TCP 活性检测！健康检测的一大基本设计原则就是尽可能真实的反馈服务本身的真实健康状态，否则一个不敢被服务调用者相信的健康状态判定结果还不如没有健康检测。注册中心的容灾考虑前文提过，在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性

在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。但是在交易场景交易链路上，在主业务数据存取，大规模服务发现、大规模健康监测等方面有天然的短板，应该竭力避免在这些场景下引入 ZooKeeper，在阿里巴巴的生产实践中，应用对 ZooKeeper 申请使用的时候要进行严格的场景、容量、SLA 需求的评估。所以可以使用 ZooKeeper，但是大数据请向左，而交易则向右，分布式协调向左，服务发现向右。


#raft
##Raft系列4 任期和选举（原创）
https://www.jianshu.com/p/aa137c94ac0b

在raft算法中，比较谁的数据最新有2个参考指标，任期和logIndex，任期大的节点，数据一定最新，任期一样的话，就要比较该任期内谁的MaxLogIndex最大了。引入任期的概念可以简化数据比较的精度。
任期的作用：

不同的服务器节点观察到的任期转换的次数可能不同，在某些情况下，一个服务器节点可能没有看到 leader 选举过程或者甚至整个任期全程。

任期在 Raft 算法中充当逻辑时钟的作用，这使得服务器节点可以发现一些过期的信息比如过时的 leader 。

每一个服务器节点存储一个当前任期号，该编号随着时间单调递增。

服务器之间通信的时候会交换当前任期号；

如果一个服务器的当前任期号比其他的小，该服务器会将自己的任期号更新为较大的那个值。

如果一个 candidate 或者 leader 发现自己的任期号过期了，它会立即回到 follower 状态。（所以说老leader如果发生了网络分区，后来接收到新leader的心跳的时候，比拼完任期之后，会自动变成follower。

如果一个节点接收到一个包含过期的任期号的请求，它会直接拒绝这个请求。

#两阶段提交协议
##关于分布式事务、两阶段提交协议、三阶提交协议
https://blog.csdn.net/aeaiesb/article/details/50343493
分布式一致性回顾
在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的增\删\改\差等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。

为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Two Phase Commitment Protocol）和Paxos算法。

分布式事务
分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）

在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。

XA规范
X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。 通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。 所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。 一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。

XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。

二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现XA分布式事务的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做)

###2PC
二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。

二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：

1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。

###3PC
三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。

与两阶段提交不同的是，三阶段提交有两个改动点。

引入超时机制。同时在协调者和参与者中都引入超时机制。
在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。
也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

CanCommit阶段
3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No
PreCommit阶段
协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。

假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。

发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。
假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。

发送中断请求 协调者向所有参与者发送abort请求。
中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。
doCommit阶段
该阶段进行真正的事务提交，也可以分为以下两种情况。

执行提交

发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
响应反馈 事务提交完之后，向协调者发送Ack响应。
完成事务 协调者接收到所有参与者的ack响应之后，完成事务。
中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

发送中断请求 协调者向所有参与者发送abort请求
事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息
中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。
在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）

###2PC与3PC的区别
相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。后面的文章会介绍这个公认为难于理解但是行之有效的Paxos算法。

参考资料：
分布式协议之两阶段提交协议（2PC）和改进三阶段提交协议（3PC）
关于分布式事务、两阶段提交、一阶段提交、Best Efforts 1PC模式和事务补偿机制的研究
两阶段提交协议与三阶段提交协议

#分布式事务
##分布式事务与一致性算法Paxos raft zab
https://blog.csdn.net/qq_44894655/article/details/89173816

说明：以下内容总结自网络
1.CAP原理
要想数据高可用，就得写多份数据
写多分数据就会导致数据一致性问题
数据一致性问题会引起性能问题

2.一致性模型
弱一致性
最终一致性（一段时间达到一致性）
强一致
1、2 异步冗余；3是同步冗余

3.  扩展服务的方案
数据分区： uid % 16
数据镜像：让多有的服务器都有相同的数据，提供相当的服务（冗余存储，一般3份为好）

4.两种方案的事务问题
A向B汇钱，两个用户不在一个服务器上
镜像：在不同的服务器上对同一数据的写操作如何保证一致性。

5. 解决一致性事务问题的技术
1. Master -Slave
读写请求由Master负责
写请求写到Master后，由Master同步到Slave上
由Master push or Slave pull
通常是由Slave 周期性来pull，所以是最终一致性
问题： 若在 pull 周期内（不是期间？），master挂掉，那么会导致这个时间片内的数据丢失
若不想让数据丢掉，Slave 只能成为 ReadOnly方式等Master恢复
若容忍数据丢失，可以让 Slave代替Master工作
如何保证强一致性？
Master 写操作，写完成功后，再写 Slave，两者成功后返回成功。若 Slave失败，两种方法
标记 Slave 不可用报错，并继续服务（等恢复后，再同步Master的数据，多个Slave少了一个而已）
回滚自己并返回失败
2. Master-Master
数据同步一般是通过 Master 间的异步完成，所以是最终一致
好处： 一台Master挂掉，另外一台照样可以提供读写服务。当数据没有被赋值到别的Master上时，数据会丢失。
对同一数据的处理问题：Dynamo的Vector Clock的设计（记录数据的版本号和修改者），当数据发生冲突时，要开发者自己来处理

3.两阶段提交  Two  Phase Commit   _ 2PC
第一阶段：针对准备工作
协调者问所有节点是否可以执行提交
参与者开始事务，执行准备工作：锁定资源（获取锁操作）
参与者响应协调者，如果事务的准备工作成功，则回应"可以提交"，否则，拒绝提交
第二阶段：
若都响应可以提交，则协调者项多有参与者发送正式提交的命令（更新值），参与者完成正式提交，释放资源，回应完成。协调者收到所有节点的完成响应后结束这个全局事务.。若参与者回应拒绝提交，则协调者向所有的参与者发送回滚操作，并释放资源，当收到全部节点的回滚回应后，取消全局事务
存在的问题：若一个没提交，就会进行回滚
第一阶段：若消息的传递未接收到，则需要协调者作超时处理，要么当做失败，要么重载
第二阶段：若参与者的回应超时，要么重试，要么把那个参与者即为问题节点，提出整个集群
在第二阶段中，参与者未收到协调者的指示（也许协调者挂掉），则所有参与者会进入“不知所措” 的状态（但是已经锁定了资源），所以引入了三段提交

4. 三段提交：把二段提交的第一阶段 break 成了两段

询问
锁定资源（获取锁）
提交
核心理念：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁定
好处：当发生了失败或超时时，三段提交可以继续把状态变为Commit 状态，而二段提交则不知所措？
5. Raxos 算法（少数服从多数）
解决的问题：在一个可能发生异常的分布式系统中如何就某个值达成一致，让整个集群的节点对某个值的变更达成一致
任何一个节点都可以提出要修改某个数据的提案,是否通过这个提案取决于这个集群中是否有超过半数的节点同意（所以节点数总是单数）—— 版本标记。虽然一致性，但是只能对一个操作进行操作啊？？
当一个Server接收到比当前版本号小的提案时，则拒绝。当收到比当前大的版本号的提案时，则锁定资源，进行修改，返回OK.   也就是说收到超过一半的最大版本的提案才算成功。

核心思想：
在抢占式访问权的基础上引入多个acceptor，也就是说当一个版本号更大的提案可以剥夺版本号已经获取的锁。
后者认同前者的原则：
在肯定旧epoch 无法生成确定性取值时，新的 epoch 会提交自己的valu
一旦 旧epoch形成确定性取值，新的 epoch肯定可以获取到此取值，并且会认同此取值，不会被破坏。

6.ZAB 协议 ( Zookeeper Atomic  Broadcast) 原子广播协议：
保证了发给各副本的消息顺序相同

定义：原子广播协议 ZAB 是一致性协议，Zookeeper 把其作为数据一致性的算法。
ZAB 是在 Paxos 算法基础上进行扩展而来的。Zookeeper 使用单一主进程 Leader用于处理客户端所有事务请求，采用 ZAB 协议将服务器状态以事务形式广播到所有 Follower 上，由于事务间可能存在着依赖关系，ZAB协议保证 Leader 广播的变更序列被顺序的处理，一个状态被处理那么它所依赖的状态也已经提前被处理

核心思想：
保证任意时刻只有一个节点是Leader，所有更新事务由Leader发起去更新所有副本 Follower，更新时用的是 两段提交协议，只要多数节点 prepare 成功，就通知他们commit。各个follower 要按当初 leader 让他们 prepare 的顺序来 apply 事务

协议状态
Looking:
系统刚启动时 或者 Leader 崩溃后正处于选举状态

Following：
Follower 节点所处的状态，Follower与 Leader处于数据同步状态

Leading：
Leader 所处状态，当前集群中有一个 Leader 为主进程

ZooKeeper启动时所有节点初始状态为Looking，这时集群会尝试选举出一个Leader节点，选举出的Leader节点切换为Leading状态；当节点发现集群中已经选举出Leader则该节点会切换到Following状态，然后和Leader节点保持同步；当Follower节点与Leader失去联系时Follower节点则会切换到Looking状态，开始新一轮选举；在ZooKeeper的整个生命周期中每个节点都会在Looking、Following、Leading状态间不断转换。

选举出Leader节点后 ZAB 进入原子广播阶段，这时Leader为和自己同步每个节点 Follower 创建一个操作序列，一个时期一个 Follower 只能和一个Leader保持同步

阶段
Election： 
在 Looking状态中选举出 Leader节点，Leader的LastZXID总是最新的（只有lastZXID的节点才有资格成为Leade,这种情况下选举出来的Leader总有最新的事务日志）。
在选举的过程中会对每个Follower节点的ZXID进行对比只有highestZXID的Follower才可能当选Leader

每个Follower都向其他节点发送选自身为Leader的Vote投票请求，等待回复；
Follower接受到的Vote如果比自身的大（ZXID更新）时则投票，并更新自身的Vote，否则拒绝投票；
每个Follower中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该Follower选为Leader，投票结束；

Discovery:
Follower 节点向准 Leader推送 FollwerInfo,该信息包含了上一周期的epoch，接受准 Leader 的 NEWLEADER 指令

Sync：
将 Follower 与 Leader的数据进行同步，由Leader发起同步指令，最终保持数据的一致性

Broadcast：
Leader广播 Proposal 与 Commit，Follower 接受 Proposal 与 commit。因为一个时刻只有一个Leader节点，若是更新请求，只能由Leader节点执行（若连到的是 Follower 节点，则需转发到Leader节点执行；读请求可以从Follower 上读取，若是要最新的数据，则还是需要在 Leader上读取）

消息广播使用了TCP协议进行通讯所有保证了接受和发送事务的顺序性。广播消息时Leader节点为每个事务Proposal分配一个全局递增的ZXID（事务ID），每个事务Proposal都按照ZXID顺序来处理（Paxos 保证不了）
Leader节点为每一个Follower节点分配一个队列按事务ZXID顺序放入到队列中，且根据队列的规则FIFO来进行事务的发送。

Recovery ：
根据Leader的事务日志对Follower 节点数据进行同步更新
同步策略：
SNAP ：如果Follower数据太老，Leader将发送快照SNAP指令给Follower同步数据；
DIFF ：Leader发送从Follolwer.lastZXID到Leader.lastZXID议案的DIFF指令给Follower同步数据；
TRUNC ：当Follower.lastZXID比Leader.lastZXID大时，Leader发送从Leader.lastZXID到Follower.lastZXID的TRUNC指令让Follower丢弃该段数据；（当老Leader在Commit前挂掉，但是已提交到本地）
Follower将所有事务都同步完成后Leader会把该节点添加到可用Follower列表中；
Follower接收Leader的NEWLEADER指令，如果该指令中epoch比当前Follower的epoch小那么Follower转到Election阶段

###7. Raft 算法
Raft 算法也是一种少数服从多数的算法，在任何时候一个服务器可以扮演以下角色之一：

Leader：负责 Client 交互 和 log 复制，同一时刻系统中最多存在一个
Follower：被动响应请求 RPC，从不主动发起请求 RPC
Candidate : 由Follower 向Leader转换的中间状态

在选举Leader的过程中，是有时间限制的，raft 将时间分为一个个 Term，可以认为是“逻辑时间”：
每个 Term中至多存在1个 Leader

某些 Term由于不止一个得到的票数一样，就会选举失败，不存在Leader。则会出现 Split Vote  ，再由候选者发出邀票

每个 Server 本地维护 currentTerm

选举过程：
自增 CurrentTerm，由Follower 转换为 Candidate，设置 votedFor 为自身，并行发起 RequestVote RPC,不断重试，直至满足下列条件之一为止：

获得超过半数的Server的投票，转换为 Leader，广播 HeatBeat
接收到 合法 Leader 的 AppendEnties RPC，转换为Follower
选举超时，没有 Server选举成功，自增 currentTerm ,重新选举

当Candidate 在等待投票结果的过程中，可能会接收到来自其他Leader的 AppendEntries RPC ,如果该 Leader 的 Term 不小于本地的 Current Term，则认可该Leader身份的合法性，主动降级为Follower，反之，则维持 candida 身份继续等待投票结果

Candidate 既没有选举成功，也没有收到其他 Leader 的 RPC (多个节点同时发起选举，最终每个 Candidate都将超时)，为了减少冲突，采取随机退让策略，每个 Candidate 重启选举定时器

